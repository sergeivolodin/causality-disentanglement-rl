{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "import os\n",
    "os.environ['CUDA_VISIBLE_DEVICES'] = \"0\"\n",
    "import numpy as np\n",
    "from matplotlib import pyplot as plt\n",
    "import seaborn as sns\n",
    "import pandas as pd\n",
    "from tqdm.auto import tqdm\n",
    "import torch\n",
    "from torch import nn\n",
    "import gin\n",
    "import pickle\n",
    "import io\n",
    "gin.enter_interactive_mode()\n",
    "from causal_util import load_env\n",
    "from causal_util.helpers import lstdct2dctlst\n",
    "from sparse_causal_model_learner_rl.loss.losses import fit_loss\n",
    "from sparse_causal_model_learner_rl.loss.causal_discriminator import siamese_feature_discriminator_l2\n",
    "\n",
    "from sparse_causal_model_learner_rl.metrics.context_rewrite import context_rewriter\n",
    "from sparse_causal_model_learner_rl.visual.learner_visual import graph_for_matrices\n",
    "from sparse_causal_model_learner_rl.config import Config\n",
    "from sparse_causal_model_learner_rl.sacred_gin_tune.sacred_wrapper import load_config_files\n",
    "from sparse_causal_model_learner_rl.learners.rl_learner import CausalModelLearnerRL"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "load_config_files(['../sparse_causal_model_learner_rl/configs/kc_rec_nonlin_gnn_gumbel_siamese_l2-Copy1.gin',\n",
    "                   '../sparse_causal_model_learner_rl/configs/kc_dec.gin',\n",
    "                   '../keychest/config/5x5_1f1c1k_obs.gin'])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "gin.bind_parameter('Config.collect_remotely', True)\n",
    "gin.bind_parameter('Config.n_collectors', 30)\n",
    "gin.bind_parameter('Config.future_buffer_size', 50)\n",
    "gin.bind_parameter('Normalizer.type_', 'meanstd')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "learner = CausalModelLearnerRL(Config(ray_kwargs={'address': '127.0.0.1:6379'}))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "learner.create_trainables()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "env = learner.env"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "losses = []\n",
    "dplus = []"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "opt = torch.optim.Adam(lr=1e-3, params=learner.decoder.parameters())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "for i in tqdm(range(1000)):\n",
    "    if i % 10 == 0:\n",
    "        ctx = learner.collect_and_get_context()\n",
    "    \n",
    "    opt.zero_grad()\n",
    "    l_obj = siamese_feature_discriminator_l2(**ctx)\n",
    "    loss = l_obj['loss']\n",
    "    loss.backward()\n",
    "    opt.step()\n",
    "    losses.append(loss.item())\n",
    "    dplus.append(l_obj['metrics']['distance_plus'])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "plt.plot(losses, label='loss')\n",
    "plt.plot(dplus, label='dplus')\n",
    "plt.yscale('log')\n",
    "plt.legend()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "np.mean(losses[-50:]), np.median(losses[-50:])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "features = learner.decoder(ctx['obs'])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "f = features.detach().cpu().numpy()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "f.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "plt.figure(figsize=(25, 10))\n",
    "for i_f in range(f.shape[1]):\n",
    "    plt.subplot(6, 5, i_f + 1)\n",
    "    plt.hist(f[:, i_f])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "pd.DataFrame(f).std()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "sns.heatmap(pd.DataFrame(f).corr())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "env.engine.return_rgb = False"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "gin.bind_parameter('model_gnn/FCCombinedModel.activation_cls', torch.nn.LeakyReLU)\n",
    "gin.bind_parameter('model_gnn/FCCombinedModel.hidden_sizes', [100, 100, 100])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "learner.non_sparse_model.__init__(feature_shape=learner.feature_shape,\n",
    "                                  action_shape=learner.action_shape,\n",
    "                                  additional_feature_shape=learner.additional_feature_shape)\n",
    "learner.non_sparse_model = learner.non_sparse_model.cuda()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "learner.non_sparse_model.model.model.activation"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "learner.non_sparse_model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "learner.non_sparse_model.model.enable_switch"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "mins = None\n",
    "maxes = None"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def normalize(t, mins, maxes):\n",
    "    return 2 * ((t - mins) / (1e-3 + maxes - mins) - 0.5)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "class AdhocWorldModel(nn.Module):\n",
    "    def __init__(self):\n",
    "        super(AdhocWorldModel, self).__init__()\n",
    "        dim = 60\n",
    "        self.fc1 = nn.Linear(in_features=34, out_features=dim)\n",
    "        self.fc2 = nn.Linear(in_features=dim, out_features=dim)\n",
    "        self.fc3 = nn.Linear(in_features=dim, out_features=dim)\n",
    "        self.fc4 = nn.Linear(in_features=dim, out_features=32)\n",
    "        self.activation = nn.Tanh()\n",
    "        \n",
    "    def forward(self, f_t, a_t, all):\n",
    "        x = torch.cat([f_t, a_t], dim=1)\n",
    "        x = self.fc1(x)\n",
    "        x = self.activation(x)\n",
    "        \n",
    "#         x = self.fc2(x)\n",
    "#         x = self.activation(x)\n",
    "        \n",
    "#         x = self.fc3(x)\n",
    "#         x = self.activation(x)\n",
    "        \n",
    "        x = self.fc4(x)\n",
    "        return x"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "losses = []\n",
    "metrics = []\n",
    "losses_emb = []\n",
    "losses_fit = []"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "use_model = learner.non_sparse_model\n",
    "#use_model = AdhocWorldModel().cuda()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "gin.bind_parameter('decoder/FCNet.hidden_sizes', [512])\n",
    "gin.bind_parameter('decoder/FCNet.activation_cls', torch.nn.LeakyReLU)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "learner.decoder.__init__(observation_shape=learner.observation_shape,\n",
    "                         feature_shape=learner.feature_shape)\n",
    "learner.decoder = learner.decoder.cuda()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "opt = torch.optim.Adam(lr=1e-3, params=list(use_model.parameters())+list(learner.decoder.parameters()))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def fit_loss(obs_x, obs_y, action_x, decoder, model, additional_feature_keys,\n",
    "             model_forward_kwargs=None,\n",
    "             fill_switch_grad=False,\n",
    "             opt_label=None,\n",
    "             divide_by_std=False,\n",
    "             std_eps=0.05,\n",
    "             **kwargs):\n",
    "    \"\"\"Ensure that the model fits the features data.\"\"\"\n",
    "\n",
    "    if model_forward_kwargs is None:\n",
    "        model_forward_kwargs = {}\n",
    "    \n",
    "    f_t1 = decoder(obs_y).detach()\n",
    "        \n",
    "    have_additional = False\n",
    "    if additional_feature_keys:\n",
    "        have_additional = True\n",
    "        add_features_y = torch.cat([kwargs[k] for k in additional_feature_keys], dim=1)\n",
    "        \n",
    "        # WARNING: zeroing output features\n",
    "#         f_t1 = torch.zeros_like(f_t1)\n",
    "        f_t1 = torch.cat([f_t1, add_features_y], dim=1)\n",
    "        \n",
    "    # detaching second part like in q-learning makes the loss jitter\n",
    "\n",
    "    f_t1_pred = model(decoder(obs_x), action_x, all=have_additional, **model_forward_kwargs)\n",
    "\n",
    "    loss = (f_t1_pred - f_t1).pow(2)\n",
    "    loss = loss.sum(1).mean()\n",
    "\n",
    "    metrics = {'mean_feature': f_t1.mean(0).detach().cpu().numpy(),\n",
    "               'std_feature': f_t1.std(0).detach().cpu().numpy(),\n",
    "               'min_feature': f_t1.min().item(),\n",
    "               'max_feature': f_t1.max().item(),\n",
    "               #'std_feature_avg': f_t1_std.detach().cpu().numpy() if f_t1_std is not None else 0.0,\n",
    "               #'inv_std_feature_avg': 1/f_t1_std.detach().cpu().numpy() if f_t1_std is not None else 0.0\n",
    "              }\n",
    "\n",
    "    return {'loss': loss,\n",
    "            'metrics': metrics}"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "[x.mean() for x in learner.decoder.state_dict().values()]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "ctx_set = False\n",
    "for i in tqdm(range(10000)):\n",
    "    if i % 1 == 0 or not ctx_set:\n",
    "        ctx_set = True\n",
    "        ctx = learner.collect_and_get_context()\n",
    "        \n",
    "#         if mins is None or maxes is None:\n",
    "#             print(\"Computing minmax\")\n",
    "#             f_all = learner.decoder(ctx['obs'])\n",
    "#             mins = f_all.min(dim=0, keepdim=True).values.detach()\n",
    "#             maxes = f_all.max(dim=0, keepdim=True).values.detach()\n",
    "        \n",
    "#         f_t1 = learner.decoder(ctx['obs_y'])#.detach()\n",
    "#         f_t = learner.decoder(ctx['obs_x'])#.detach()\n",
    "        \n",
    "# #         f_t1 = normalize(f_t1, mins, maxes)\n",
    "# #         f_t = normalize(f_t, mins, maxes)\n",
    "        \n",
    "#         add_features_y = torch.cat([ctx[k] for k in ctx['additional_feature_keys']], dim=1)\n",
    "#         f_t1 = torch.cat([f_t1, add_features_y], dim=1)\n",
    "        \n",
    "#     f_t1_pred = use_model(\n",
    "#         f_t,\n",
    "#         ctx['action_x'],\n",
    "#         all=True)\n",
    "    \n",
    "    ctx['non_sparse_model_add'] = use_model\n",
    "#     ctx['additional_feature_keys'] = []\n",
    "    l_obj_emb = siamese_feature_discriminator_l2(**ctx)\n",
    "    l_obj_fit = context_rewriter(function=fit_loss, rewrite={'model': 'non_sparse_model_add'},\n",
    "                                 divide_by_std=False,\n",
    "                                 **ctx)\n",
    "    loss = l_obj_fit['loss'] #+ l_obj_emb['loss']\n",
    "\n",
    "    \n",
    "#     loss = nn.MSELoss()(f_t1, f_t1_pred)\n",
    "\n",
    "    \n",
    "    opt.zero_grad()\n",
    "    loss.backward()\n",
    "    opt.step()\n",
    "    losses.append(loss.item())\n",
    "    losses_emb.append(l_obj_emb['loss'].item())\n",
    "    losses_fit.append(l_obj_fit['loss'].item())\n",
    "    m = {}\n",
    "    m.update(l_obj_fit['metrics'])\n",
    "    m.update(l_obj_emb['metrics'])\n",
    "    metrics.append(m)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "plt.plot(losses, label='loss')\n",
    "# plt.plot(losses_emb, label='emb')\n",
    "plt.plot(losses_fit, label='fit')\n",
    "#plt.plot(dplus, label='dplus')\n",
    "plt.yscale('log')\n",
    "plt.legend()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "f_t1 = learner.decoder(ctx['obs_y'])#.detach()\n",
    "f_t = learner.decoder(ctx['obs_x'])#.detach()\n",
    "\n",
    "#         f_t1 = normalize(f_t1, mins, maxes)\n",
    "#         f_t = normalize(f_t, mins, maxes)\n",
    "\n",
    "add_features_y = torch.cat([ctx[k] for k in ctx['additional_feature_keys']], dim=1)\n",
    "f_t1 = torch.zeros_like(f_t1)\n",
    "f_t1 = torch.cat([f_t1, add_features_y], dim=1)\n",
    "\n",
    "f_t1_pred = use_model(\n",
    "f_t,\n",
    "ctx['action_x'],\n",
    "all=True)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "sns.heatmap((f_t1_pred - f_t1).pow(2).mean(0).cpu().detach().numpy().reshape(-1, 1))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "metrics_dct = lstdct2dctlst(metrics)\n",
    "for key, vals in metrics_dct.items():\n",
    "    if len(np.array(vals).shape) > 1: continue\n",
    "    plt.plot(vals, label=key)\n",
    "plt.legend()\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "metrics_dct.keys()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "losses[-1]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "learner.decoder(ctx['obs_x']).min()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.0"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
