{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "import os\n",
    "#os.environ['CUDA_VISIBLE_DEVICES'] = \"-1\"\n",
    "\n",
    "import pickle\n",
    "from keychest.keychestenv_gofa import features_for_obs\n",
    "from copy import deepcopy\n",
    "import pandas as pd\n",
    "import torch\n",
    "import numpy as np\n",
    "from matplotlib import pyplot as plt\n",
    "import seaborn as sns\n",
    "import gin\n",
    "from tqdm import tqdm\n",
    "from functools import partial\n",
    "from keychest.keychestenv import keychest_obs2d_to_image, keychest_obs3d_to_obs2d"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "# loading episodes\n",
    "fn = 'episodes-1000-config-10x10.gin-105a835e-fc48-11ea-a584-7b4c3c93968e.pkl'\n",
    "fn = 'keychest/' + fn\n",
    "data = pickle.load(open(fn, 'rb'))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(21, 12, 10)"
      ]
     },
     "execution_count": 3,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "data[0][0]['observation'].shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "config_fn = fn.split('-')[3]\n",
    "gin.enter_interactive_mode()\n",
    "gin.parse_config_file(f\"keychest/config/{config_fn}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "def one_hot_encode(value, entries):\n",
    "    \"\"\"Encode a value as 1-hot.\"\"\"\n",
    "    assert 0 <= value < entries\n",
    "    result = np.zeros(entries, dtype=np.float32)\n",
    "    result[value] = 1.0\n",
    "    return result"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "ACTION_DIM = 4\n",
    "one_hot_encode_action = partial(one_hot_encode, entries=ACTION_DIM)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [],
   "source": [
    "# observation, action 1-hot\n",
    "dataset_x = []\n",
    "\n",
    "# reward, next observation, done\n",
    "dataset_y = []\n",
    "\n",
    "for episode in data:        \n",
    "    prev_observation = episode[0]['observation']\n",
    "    \n",
    "    for i, step in enumerate(episode[1:]):\n",
    "        dataset_y.append({})\n",
    "        dataset_x.append({})\n",
    "        \n",
    "        dataset_x[-1]['action'] = one_hot_encode_action(step['action'])\n",
    "        dataset_x[-1]['observation'] = prev_observation.transpose(2, 0, 1)\n",
    "        dataset_y[-1]['observation'] = step['observation'].transpose(2, 0, 1)\n",
    "        dataset_y[-1]['reward'] = step['reward']\n",
    "        dataset_y[-1]['done'] = step['done']\n",
    "        \n",
    "        prev_observation = step['observation']\n",
    "        \n",
    "        \n",
    "assert len(dataset_x) == len(dataset_y)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [],
   "source": [
    "assert np.max(dataset_x[0]['observation']) == 1\n",
    "assert dataset_x[0]['observation'].shape[0] == 10"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(10, 21, 12)"
      ]
     },
     "execution_count": 9,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "dataset_x[0]['observation'].shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [],
   "source": [
    "device = torch.device(\"cuda\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "device(type='cuda')"
      ]
     },
     "execution_count": 11,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "device"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "ConvAutoencoder(\n",
      "  (conv1): Conv2d(10, 16, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1))\n",
      "  (conv2): Conv2d(16, 4, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1))\n",
      "  (pool): MaxPool2d(kernel_size=2, stride=2, padding=0, dilation=1, ceil_mode=False)\n",
      "  (t_conv1): ConvTranspose2d(65, 16, kernel_size=(10, 6), stride=(2, 2))\n",
      "  (t_conv2): ConvTranspose2d(16, 10, kernel_size=(3, 2), stride=(2, 2))\n",
      "  (act_fc1): Linear(in_features=4, out_features=15, bias=True)\n",
      "  (act_fc2): Linear(in_features=15, out_features=5, bias=True)\n",
      "  (rew_fc1): Linear(in_features=65, out_features=15, bias=True)\n",
      "  (rew_fc2): Linear(in_features=15, out_features=1, bias=True)\n",
      "  (done_fc1): Linear(in_features=65, out_features=15, bias=True)\n",
      "  (done_fc2): Linear(in_features=15, out_features=1, bias=True)\n",
      ")\n"
     ]
    }
   ],
   "source": [
    "import torch.nn as nn\n",
    "import torch.nn.functional as F\n",
    "\n",
    "# define the NN architecture\n",
    "class ConvAutoencoder(nn.Module):\n",
    "    def __init__(self):\n",
    "        super(ConvAutoencoder, self).__init__()\n",
    "        ## encoder layers ##\n",
    "        # conv layer (depth from 3 --> 16), 3x3 kernels\n",
    "        self.conv1 = nn.Conv2d(10, 16, 3, padding=1)  \n",
    "        # conv layer (depth from 16 --> 4), 3x3 kernels\n",
    "        self.conv2 = nn.Conv2d(16, 4, 3, padding=1)\n",
    "        # pooling layer to reduce x-y dims by two; kernel and stride of 2\n",
    "        self.pool = nn.MaxPool2d(2, 2)\n",
    "        \n",
    "        ## decoder layers ##\n",
    "        ## a kernel of 2 and a stride of 2 will increase the spatial dims by 2\n",
    "        self.t_conv1 = nn.ConvTranspose2d(65, 16, (10, 6), stride=2)\n",
    "        self.t_conv2 = nn.ConvTranspose2d(16, 10, (3, 2), stride=2)\n",
    "        \n",
    "        # action encoder\n",
    "        self.act_fc1 = torch.nn.Linear(4, 15)\n",
    "        self.act_fc2 = torch.nn.Linear(15, 5)\n",
    "        \n",
    "        self.rew_fc1 = torch.nn.Linear(65, 15)\n",
    "        self.rew_fc2 = torch.nn.Linear(15, 1)\n",
    "        \n",
    "        self.done_fc1 = torch.nn.Linear(65, 15)\n",
    "        self.done_fc2 = torch.nn.Linear(15, 1)\n",
    "        \n",
    "        \n",
    "\n",
    "    def encoder(self, x):\n",
    "        code_obs = self.conv_encoder(x['observation'])\n",
    "        code_action = self.action_encoder(x['action'])\n",
    "        return torch.cat((code_obs, code_action), 1)\n",
    "    \n",
    "    def conv_encoder_noflat(self, x):\n",
    "        x = F.relu(self.conv1(x))\n",
    "        x = self.pool(x)\n",
    "        # add second hidden layer\n",
    "        x = F.relu(self.conv2(x))\n",
    "        x = self.pool(x)  # compressed representation\n",
    "        return x\n",
    "    \n",
    "    def conv_encoder(self, x):\n",
    "        x = self.conv_encoder_noflat(x)\n",
    "        x = torch.flatten(x, start_dim=1)\n",
    "        return x\n",
    "    \n",
    "    def action_encoder(self, x):\n",
    "        x = F.relu(self.act_fc1(x))\n",
    "        x = F.relu(self.act_fc2(x))\n",
    "        return x\n",
    "        \n",
    "    def decode_obs(self, x):\n",
    "        ## decode ##\n",
    "        # add transpose conv layers, with relu activation function\n",
    "        x = x.view(x.size(0), x.size(1), 1, 1)\n",
    "        x = F.relu(self.t_conv1(x))\n",
    "        # output layer (with sigmoid for scaling from 0 to 1)\n",
    "        x = F.sigmoid(self.t_conv2(x))\n",
    "        return x\n",
    "        \n",
    "    def decode_reward(self, x):\n",
    "        x = torch.flatten(x, start_dim=1)\n",
    "        x = F.relu(self.rew_fc1(x))\n",
    "        x = self.rew_fc2(x)\n",
    "        return x.view(x.size(0))\n",
    "    \n",
    "    def decode_done(self, x):\n",
    "        x = torch.flatten(x, start_dim=1)\n",
    "        x = F.relu(self.done_fc1(x))\n",
    "        x = self.done_fc2(x)\n",
    "        return x.view(x.size(0))\n",
    "        \n",
    "    def forward(self, x):\n",
    "        code = self.encoder(x)\n",
    "        \n",
    "        new_obs = self.decode_obs(code)\n",
    "        reward = self.decode_reward(code)\n",
    "        done = self.decode_done(code)\n",
    "        result = {'observation': new_obs,\n",
    "                  'reward': reward,\n",
    "                  'done': done}\n",
    "        return result\n",
    "\n",
    "# initialize the NN\n",
    "model = ConvAutoencoder()\n",
    "print(model)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [],
   "source": [
    "data = {x: torch.from_numpy(np.array([y])) for x, y in dataset_x[0].items()}"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/home/sergei/miniconda3/envs/ml/lib/python3.7/site-packages/torch/nn/functional.py:1625: UserWarning: nn.functional.sigmoid is deprecated. Use torch.sigmoid instead.\n",
      "  warnings.warn(\"nn.functional.sigmoid is deprecated. Use torch.sigmoid instead.\")\n"
     ]
    }
   ],
   "source": [
    "decoded = model(data)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [],
   "source": [
    "model = model.to(device)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [],
   "source": [
    "assert decoded['reward'].shape == (1,)\n",
    "assert decoded['done'].shape == (1,)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {},
   "outputs": [],
   "source": [
    "assert decoded['observation'].shape == data['observation'].shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {},
   "outputs": [],
   "source": [
    "def dct_fcn(dct, fcn):\n",
    "    return {x: fcn(y) for x, y in dct.items()}\n",
    "\n",
    "def dct_np(dct):\n",
    "    return dct_fcn(dct, partial(np.array, dtype=np.float32))\n",
    "\n",
    "def dct_torch(dct):\n",
    "    return dct_fcn(dct, lambda x: torch.from_numpy(x))\n",
    "\n",
    "def lstdct2dctlst(lst):\n",
    "    result = {}\n",
    "    for item in lst:\n",
    "        for key, value in item.items():\n",
    "            if key not in result:\n",
    "                result[key] = []\n",
    "            result[key].append(value)\n",
    "    return dct_np(result)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {},
   "outputs": [],
   "source": [
    "dataset_x_dct = dct_torch(lstdct2dctlst(dataset_x))\n",
    "dataset_y_dct = dct_torch(lstdct2dctlst(dataset_y))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "metadata": {},
   "outputs": [],
   "source": [
    "p = np.random.permutation(len(dataset_x_dct['observation']))\n",
    "#return a[p], b[p]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "metadata": {},
   "outputs": [],
   "source": [
    "dataset_x_dct_train = dct_fcn(dataset_x_dct, lambda x: x[p])\n",
    "dataset_y_dct_train = dct_fcn(dataset_y_dct, lambda x: x[p])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "torch.Size([22350, 10, 21, 12])"
      ]
     },
     "execution_count": 22,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "dataset_x_dct['observation'].shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "metadata": {},
   "outputs": [],
   "source": [
    "#model_pred = model(dataset_x_dct)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "metadata": {},
   "outputs": [],
   "source": [
    "optimizer = torch.optim.Adam(model.parameters(), lr=1e-3)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "metadata": {},
   "outputs": [],
   "source": [
    "loss_mse = nn.MSELoss()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 60,
   "metadata": {},
   "outputs": [],
   "source": [
    "def loss_rl_pred(data_pred, data_y):\n",
    "    result = 0\n",
    "    # keys = data_pred.keys()\n",
    "    keys = ['observation']\n",
    "    for key in keys:\n",
    "        result += loss_mse(data_pred[key], data_y[key])\n",
    "    return result"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 61,
   "metadata": {},
   "outputs": [],
   "source": [
    "#del model\n",
    "#torch.cuda.empty_cache()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 62,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "6it [00:00, 12.05it/s, loss=0.0075170957]                       \n",
      "6it [00:00, 13.56it/s, loss=0.007513974]                        \n",
      "6it [00:00, 13.64it/s, loss=0.0075109378]                       \n",
      "6it [00:00, 13.82it/s, loss=0.007507676]                        \n",
      "6it [00:00, 13.94it/s, loss=0.0075046485]                       \n",
      "6it [00:00, 13.49it/s, loss=0.0075016]                          \n",
      "6it [00:00, 13.50it/s, loss=0.0074985996]                       \n",
      "6it [00:00, 13.35it/s, loss=0.007495701]                        \n",
      "6it [00:00, 13.26it/s, loss=0.007492953]                        \n",
      "6it [00:00, 13.65it/s, loss=0.007490234]                        \n",
      "6it [00:00, 13.17it/s, loss=0.0074876356]                       \n",
      "6it [00:00, 13.78it/s, loss=0.0074851406]                       \n",
      "6it [00:00, 13.70it/s, loss=0.0074827317]                       \n",
      "6it [00:00, 13.78it/s, loss=0.0074804123]                       \n",
      "6it [00:00, 13.86it/s, loss=0.007478166]                        \n",
      "6it [00:00, 13.60it/s, loss=0.0074759806]                       \n",
      "6it [00:00, 13.91it/s, loss=0.007473852]                        \n",
      "6it [00:00, 13.77it/s, loss=0.0074717733]                       \n",
      "6it [00:00, 13.34it/s, loss=0.0074697374]                       \n",
      "6it [00:00, 13.52it/s, loss=0.007467736]                        \n",
      "6it [00:00, 13.76it/s, loss=0.0074657607]                       \n",
      "6it [00:00, 12.64it/s, loss=0.0074638072]                       \n",
      "6it [00:00, 13.51it/s, loss=0.0074618766]                       \n",
      "6it [00:00, 13.19it/s, loss=0.007459989]                        \n",
      "6it [00:00, 13.42it/s, loss=0.007458172]                        \n",
      "6it [00:00, 13.50it/s, loss=0.0074564363]                       \n",
      "6it [00:00, 12.72it/s, loss=0.0074547725]                       \n",
      "6it [00:00, 13.81it/s, loss=0.0074531655]                       \n",
      "6it [00:00, 13.44it/s, loss=0.0074516144]                       \n",
      "6it [00:00, 13.26it/s, loss=0.0074501154]                       \n",
      "6it [00:00, 13.35it/s, loss=0.0074486667]                       \n",
      "6it [00:00, 13.41it/s, loss=0.007447255]                        \n",
      "6it [00:00, 13.40it/s, loss=0.007445871]                        \n",
      "6it [00:00, 13.50it/s, loss=0.007444511]                        \n",
      "6it [00:00, 13.84it/s, loss=0.00744317]                         \n",
      "6it [00:00, 13.45it/s, loss=0.007441841]                        \n",
      "6it [00:00, 13.62it/s, loss=0.0074405284]                       \n",
      "6it [00:00, 13.86it/s, loss=0.0074392324]                       \n",
      "6it [00:00, 13.68it/s, loss=0.0074379584]                       \n",
      "6it [00:00, 13.11it/s, loss=0.0074367034]                       \n",
      "6it [00:00, 13.29it/s, loss=0.007435466]                        \n",
      "6it [00:00, 12.73it/s, loss=0.0074342475]                       \n",
      "6it [00:00, 13.37it/s, loss=0.0074330354]                       \n",
      "6it [00:00, 13.04it/s, loss=0.007431824]                       \n",
      "6it [00:00, 12.76it/s, loss=0.0074306256]                       \n",
      "6it [00:00, 13.80it/s, loss=0.0074294414]                       \n",
      "6it [00:00, 13.55it/s, loss=0.007428262]                        \n",
      "6it [00:00, 13.81it/s, loss=0.0074270987]                       \n",
      "6it [00:00, 13.35it/s, loss=0.0074259513]                       \n",
      "6it [00:00, 13.54it/s, loss=0.007424824]                        \n"
     ]
    }
   ],
   "source": [
    "epochs = 50\n",
    "batch_size = 4096\n",
    "\n",
    "\n",
    "for epoch in range(epochs):\n",
    "    with tqdm(total=len(dataset_x_dct_train['observation']) // batch_size) as pbar:\n",
    "        for i, batch in enumerate(range(0, len(dataset_x_dct_train['observation']), batch_size)):\n",
    "            x = dct_fcn(dataset_x_dct_train, lambda x: x[batch : batch + batch_size].to(device))\n",
    "            y = dct_fcn(dataset_y_dct_train, lambda x: x[batch : batch + batch_size].to(device))\n",
    "\n",
    "            if not x['observation'].shape[0]: continue\n",
    "\n",
    "            out = model(x)\n",
    "            loss = loss_rl_pred(out, y)\n",
    "            optimizer.zero_grad()\n",
    "            loss.backward()\n",
    "            optimizer.step()\n",
    "\n",
    "\n",
    "            #to_print = \"Epoch[{}/{}] Loss: {:.3f}\".format(epoch + 1, epochs, loss.data)\n",
    "            #print(to_print)\n",
    "            pbar.set_postfix(loss=loss.data.cpu().numpy())\n",
    "            pbar.update(1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 63,
   "metadata": {},
   "outputs": [],
   "source": [
    "batch = 0\n",
    "x = dct_fcn(dataset_x_dct_train, lambda x: x[batch * batch_size : (batch + 1) * batch_size].to(device))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 64,
   "metadata": {},
   "outputs": [],
   "source": [
    "pred = model(x)['observation'][0].detach().cpu().numpy().transpose(1, 2, 0)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 65,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "<matplotlib.image.AxesImage at 0x7f154d8aa690>"
      ]
     },
     "execution_count": 65,
     "metadata": {},
     "output_type": "execute_result"
    },
    {
     "data": {
      "image/png": "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\n",
      "text/plain": [
       "<Figure size 432x288 with 1 Axes>"
      ]
     },
     "metadata": {
      "needs_background": "light"
     },
     "output_type": "display_data"
    }
   ],
   "source": [
    "plt.imshow(keychest_obs2d_to_image(keychest_obs3d_to_obs2d(1 * (pred > 0.5))))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 66,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "<matplotlib.image.AxesImage at 0x7f154d714190>"
      ]
     },
     "execution_count": 66,
     "metadata": {},
     "output_type": "execute_result"
    },
    {
     "data": {
      "image/png": "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\n",
      "text/plain": [
       "<Figure size 432x288 with 1 Axes>"
      ]
     },
     "metadata": {
      "needs_background": "light"
     },
     "output_type": "display_data"
    }
   ],
   "source": [
    "plt.imshow(keychest_obs2d_to_image(keychest_obs3d_to_obs2d(dataset_x_dct_train['observation'][0].numpy().transpose(1, 2, 0))))"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.0"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
