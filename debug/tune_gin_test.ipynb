{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sparse_causal_model_learner_rl.sacred_gin_tune.sacred_wrapper import load_config_files\n",
    "from sparse_causal_model_learner_rl.sacred_gin_tune.tune_to_gin import tune_grid_search\n",
    "from sparse_causal_model_learner_rl.sacred_gin_tune.test_load_gin_file import f, g\n",
    "\n",
    "\n",
    "import gin\n",
    "import tempfile\n",
    "import pytest\n",
    "import os\n",
    "from ray import tune"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {},
   "outputs": [],
   "source": [
    "def g_conf_grid_search():\n",
    "    conf_file = \"\"\"\n",
    "        import sparse_causal_model_learner_rl.sacred_gin_tune.test_load_gin_file\n",
    "        import sparse_causal_model_learner_rl.sacred_gin_tune.tune_to_gin\n",
    "        \n",
    "        ABA = 'caba'\n",
    "        \n",
    "        scope1/test_load_gin_file.f.param = 123\n",
    "        scope2/test_load_gin_file.f.param = 456\n",
    "        \n",
    "        tune1/tune_grid_search.values = [@scope1/test_load_gin_file.f(), @scope2/test_load_gin_file.f()]\n",
    "        tune2/tune_grid_search.values = [%ABA, @test_load_gin_file.f]\n",
    "        \n",
    "        test_load_gin_file.g.param1 = @tune1/tune_grid_search()\n",
    "        test_load_gin_file.g.param2 = @tune2/tune_grid_search()\n",
    "        \"\"\"\n",
    "    return conf_file\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "['tmp2dr3']"
      ]
     },
     "execution_count": 18,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "with tempfile.NamedTemporaryFile(mode='w+', delete=False) as tmp_config_file:\n",
    "    tmp_config = tmp_config_file.name\n",
    "    tmp_config_file.write(g_conf_grid_search())\n",
    "\n",
    "load_config_files([tmp_config])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "module_fcn_name = tune_grid_search.__module__ + \".\" + tune_grid_search.__name__\n",
    "fcn_name = tune_grid_search.__name__"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 70,
   "metadata": {},
   "outputs": [],
   "source": [
    "search = {}\n",
    "for (scope, fcn), args in gin.config._CONFIG.items():\n",
    "    if fcn == module_fcn_name:\n",
    "        search[f\"{scope}/{fcn_name}\"] = tune.grid_search(args['values'])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 73,
   "metadata": {},
   "outputs": [],
   "source": [
    "def fcn(config):\n",
    "    gin.parse_config(config['gin_config'])\n",
    "\n",
    "    \n",
    "    for key, value in config.items():\n",
    "        if key.endswith('grid_search'):\n",
    "            scope, name = key.split('/')\n",
    "            gin.bind_parameter(scope + '/' + name + '.override', value)\n",
    "            print(\"bind\", key, value, scope, name)\n",
    "    \n",
    "    \n",
    "    res = g()\n",
    "    print(res)\n",
    "    return res"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 74,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "== Status ==<br>Memory usage on this node: 0.9/8.7 GiB<br>Using FIFO scheduling algorithm.<br>Resources requested: 1/10 CPUs, 0/0 GPUs, 0.0/5.03 GiB heap, 0.0/1.71 GiB objects<br>Result logdir: /home/sergei/ray_results/fcn<br>Number of trials: 4 (3 PENDING, 1 RUNNING)<br><table>\n",
       "<thead>\n",
       "<tr><th>Trial name     </th><th>status  </th><th>loc  </th><th>tune1/tune_grid_search  </th><th>tune2/tune_grid_search  </th></tr>\n",
       "</thead>\n",
       "<tbody>\n",
       "<tr><td>fcn_39b12_00000</td><td>RUNNING </td><td>     </td><td>                        </td><td>                        </td></tr>\n",
       "<tr><td>fcn_39b12_00001</td><td>PENDING </td><td>     </td><td>                        </td><td>                        </td></tr>\n",
       "<tr><td>fcn_39b12_00002</td><td>PENDING </td><td>     </td><td>                        </td><td>                        </td></tr>\n",
       "<tr><td>fcn_39b12_00003</td><td>PENDING </td><td>     </td><td>                        </td><td>                        </td></tr>\n",
       "</tbody>\n",
       "</table><br><br>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\u001b[2m\u001b[36m(pid=22407)\u001b[0m bind tune1/tune_grid_search 123 tune1 tune_grid_search\n",
      "\u001b[2m\u001b[36m(pid=22407)\u001b[0m bind tune2/tune_grid_search caba tune2 tune_grid_search\n",
      "\u001b[2m\u001b[36m(pid=22407)\u001b[0m (123, 'caba')\n",
      "\u001b[2m\u001b[36m(pid=22405)\u001b[0m bind tune1/tune_grid_search 456 tune1 tune_grid_search\n",
      "\u001b[2m\u001b[36m(pid=22405)\u001b[0m bind tune2/tune_grid_search caba tune2 tune_grid_search\n",
      "\u001b[2m\u001b[36m(pid=22405)\u001b[0m (456, 'caba')\n",
      "\u001b[2m\u001b[36m(pid=22418)\u001b[0m bind tune1/tune_grid_search 123 tune1 tune_grid_search\n",
      "\u001b[2m\u001b[36m(pid=22418)\u001b[0m bind tune2/tune_grid_search <function f at 0x7fca42a399d8> tune2 tune_grid_search\n",
      "\u001b[2m\u001b[36m(pid=22418)\u001b[0m (123, <function f at 0x7fca42a399d8>)\n"
     ]
    },
    {
     "data": {
      "text/html": [
       "== Status ==<br>Memory usage on this node: 0.9/8.7 GiB<br>Using FIFO scheduling algorithm.<br>Resources requested: 0/10 CPUs, 0/0 GPUs, 0.0/5.03 GiB heap, 0.0/1.71 GiB objects<br>Result logdir: /home/sergei/ray_results/fcn<br>Number of trials: 4 (4 TERMINATED)<br><table>\n",
       "<thead>\n",
       "<tr><th>Trial name     </th><th>status    </th><th>loc  </th><th>tune1/tune_grid_search  </th><th>tune2/tune_grid_search  </th></tr>\n",
       "</thead>\n",
       "<tbody>\n",
       "<tr><td>fcn_39b12_00000</td><td>TERMINATED</td><td>     </td><td>                        </td><td>                        </td></tr>\n",
       "<tr><td>fcn_39b12_00001</td><td>TERMINATED</td><td>     </td><td>                        </td><td>                        </td></tr>\n",
       "<tr><td>fcn_39b12_00002</td><td>TERMINATED</td><td>     </td><td>                        </td><td>                        </td></tr>\n",
       "<tr><td>fcn_39b12_00003</td><td>TERMINATED</td><td>     </td><td>                        </td><td>                        </td></tr>\n",
       "</tbody>\n",
       "</table><br><br>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\u001b[2m\u001b[36m(pid=22527)\u001b[0m bind tune1/tune_grid_search 456 tune1 tune_grid_search\n",
      "\u001b[2m\u001b[36m(pid=22527)\u001b[0m bind tune2/tune_grid_search <function f at 0x7f2e00ad49d8> tune2 tune_grid_search\n",
      "\u001b[2m\u001b[36m(pid=22527)\u001b[0m (456, <function f at 0x7f2e00ad49d8>)\n"
     ]
    }
   ],
   "source": [
    "#search['num_workers'] = 0\n",
    "search['gin_config'] = gin.config_str()\n",
    "res = tune.run(fcn, config=search)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 51,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "['__class__',\n",
       " '__delattr__',\n",
       " '__dict__',\n",
       " '__dir__',\n",
       " '__doc__',\n",
       " '__eq__',\n",
       " '__format__',\n",
       " '__ge__',\n",
       " '__getattribute__',\n",
       " '__gt__',\n",
       " '__hash__',\n",
       " '__init__',\n",
       " '__init_subclass__',\n",
       " '__le__',\n",
       " '__lt__',\n",
       " '__module__',\n",
       " '__ne__',\n",
       " '__new__',\n",
       " '__reduce__',\n",
       " '__reduce_ex__',\n",
       " '__repr__',\n",
       " '__setattr__',\n",
       " '__sizeof__',\n",
       " '__str__',\n",
       " '__subclasshook__',\n",
       " '__weakref__',\n",
       " '_checkpoints',\n",
       " '_configs',\n",
       " '_experiment_dir',\n",
       " '_experiment_state',\n",
       " '_get_trial_paths',\n",
       " '_retrieve_rows',\n",
       " '_trial_dataframes',\n",
       " '_validate_metric',\n",
       " '_validate_mode',\n",
       " 'best_checkpoint',\n",
       " 'best_config',\n",
       " 'best_dataframe',\n",
       " 'best_logdir',\n",
       " 'best_result',\n",
       " 'best_result_df',\n",
       " 'best_trial',\n",
       " 'dataframe',\n",
       " 'default_metric',\n",
       " 'default_mode',\n",
       " 'fetch_trial_dataframes',\n",
       " 'get_all_configs',\n",
       " 'get_best_checkpoint',\n",
       " 'get_best_config',\n",
       " 'get_best_logdir',\n",
       " 'get_best_trial',\n",
       " 'get_trial_checkpoints_paths',\n",
       " 'results',\n",
       " 'results_df',\n",
       " 'runner_data',\n",
       " 'stats',\n",
       " 'trial_dataframes',\n",
       " 'trials']"
      ]
     },
     "execution_count": 51,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "dir(res)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 47,
   "metadata": {},
   "outputs": [
    {
     "ename": "ValueError",
     "evalue": "No `metric` has been passed and  `default_metric` has not been set. Please specify the `metric` parameter.",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mValueError\u001b[0m                                Traceback (most recent call last)",
      "\u001b[0;32m<ipython-input-47-6556b1e4e3d8>\u001b[0m in \u001b[0;36m<module>\u001b[0;34m\u001b[0m\n\u001b[0;32m----> 1\u001b[0;31m \u001b[0mres\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mdataframe\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m",
      "\u001b[0;32m~/miniconda3/envs/causal/lib/python3.7/site-packages/ray/tune/analysis/experiment_analysis.py\u001b[0m in \u001b[0;36mdataframe\u001b[0;34m(self, metric, mode)\u001b[0m\n\u001b[1;32m     87\u001b[0m             \u001b[0mpd\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mDataFrame\u001b[0m\u001b[0;34m:\u001b[0m \u001b[0mConstructed\u001b[0m \u001b[0;32mfrom\u001b[0m \u001b[0ma\u001b[0m \u001b[0mresult\u001b[0m \u001b[0mdict\u001b[0m \u001b[0mof\u001b[0m \u001b[0meach\u001b[0m \u001b[0mtrial\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     88\u001b[0m         \"\"\"\n\u001b[0;32m---> 89\u001b[0;31m         \u001b[0mmetric\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_validate_metric\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mmetric\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m     90\u001b[0m         \u001b[0mmode\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_validate_mode\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mmode\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     91\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m~/miniconda3/envs/causal/lib/python3.7/site-packages/ray/tune/analysis/experiment_analysis.py\u001b[0m in \u001b[0;36m_validate_metric\u001b[0;34m(self, metric)\u001b[0m\n\u001b[1;32m     63\u001b[0m         \u001b[0;32mif\u001b[0m \u001b[0;32mnot\u001b[0m \u001b[0mmetric\u001b[0m \u001b[0;32mand\u001b[0m \u001b[0;32mnot\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mdefault_metric\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     64\u001b[0m             raise ValueError(\n\u001b[0;32m---> 65\u001b[0;31m                 \u001b[0;34m\"No `metric` has been passed and  `default_metric` has \"\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m     66\u001b[0m                 \"not been set. Please specify the `metric` parameter.\")\n\u001b[1;32m     67\u001b[0m         \u001b[0;32mreturn\u001b[0m \u001b[0mmetric\u001b[0m \u001b[0;32mor\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mdefault_metric\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;31mValueError\u001b[0m: No `metric` has been passed and  `default_metric` has not been set. Please specify the `metric` parameter."
     ]
    }
   ],
   "source": [
    "res.dataframe()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "()"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
