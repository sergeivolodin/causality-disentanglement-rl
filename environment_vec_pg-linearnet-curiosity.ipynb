{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "import os\n",
    "os.environ[\"CUDA_VISIBLE_DEVICES\"]=\"1\"\n",
    "os.environ[\"TF_FORCE_GPU_ALLOW_GROWTH\"]=\"true\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "from matplotlib import pyplot as plt\n",
    "import tensorflow as tf\n",
    "from tqdm import tqdm\n",
    "tf.compat.v1.enable_v2_behavior()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "from tf_agents.agents.reinforce import reinforce_agent\n",
    "from tf_agents.drivers import dynamic_step_driver\n",
    "from tf_agents.environments import suite_gym\n",
    "from tf_agents.environments import tf_py_environment\n",
    "from tf_agents.eval import metric_utils\n",
    "from tf_agents.metrics import tf_metrics, py_metrics\n",
    "from tf_agents.policies import random_tf_policy, epsilon_greedy_policy\n",
    "from tf_agents.replay_buffers import tf_uniform_replay_buffer\n",
    "from tf_agents.networks import actor_distribution_network\n",
    "from tf_agents.trajectories import trajectory\n",
    "from tf_agents.utils import common\n",
    "from tf_agents.specs import array_spec\n",
    "from tf_agents.environments import utils, wrappers\n",
    "from tf_agents.trajectories import time_step as ts\n",
    "from tf_agents.drivers import dynamic_episode_driver\n",
    "from tf_agents.drivers import py_driver"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "from vectorincrement import *"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Running RL with tf.agents"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 280,
   "metadata": {},
   "outputs": [],
   "source": [
    "num_iterations = 500 # @param {type:\"integer\"}\n",
    "collect_episodes_per_iteration = 5 # @param {type:\"integer\"}\n",
    "replay_buffer_capacity = 1000 # @param {type:\"integer\"}\n",
    "\n",
    "fc_layer_params = ()\n",
    "\n",
    "learning_rate = 1e-3 # @param {type:\"number\"}\n",
    "log_interval = 25 # @param {type:\"integer\"}\n",
    "num_eval_episodes = 10 # @param {type:\"integer\"}\n",
    "eval_interval = 10 # @param {type:\"integer\"}"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 281,
   "metadata": {},
   "outputs": [],
   "source": [
    "v_n = 2\n",
    "v_k = 2\n",
    "v_seed = 43\n",
    "do_transform = True\n",
    "time_limit = 20"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 321,
   "metadata": {},
   "outputs": [],
   "source": [
    "decoder = tf.keras.Sequential(\n",
    "    [\n",
    "        tf.keras.layers.Dense(v_n, input_shape=(v_k,), activation=None,\n",
    "                             use_bias=False, kernel_initializer='random_normal')\n",
    "    ]\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 282,
   "metadata": {},
   "outputs": [],
   "source": [
    "def get_env():\n",
    "    \"\"\"Return a copy of the environment.\"\"\"\n",
    "    env = VectorIncrementEnvironmentTFAgents(v_n=v_n, v_k=v_k, v_seed=v_seed,\n",
    "                                             do_transform=do_transform)\n",
    "    env = wrappers.TimeLimit(env, time_limit)\n",
    "    env = tf_py_environment.TFPyEnvironment(env)\n",
    "    return env"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 283,
   "metadata": {},
   "outputs": [],
   "source": [
    "train_env = get_env()\n",
    "eval_env = get_env()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 284,
   "metadata": {},
   "outputs": [],
   "source": [
    "actor_net = actor_distribution_network.ActorDistributionNetwork(\n",
    "    train_env.observation_spec(),\n",
    "    train_env.action_spec(),\n",
    "    fc_layer_params=fc_layer_params,\n",
    "    activation_fn=tf.keras.activations.relu)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 285,
   "metadata": {},
   "outputs": [],
   "source": [
    "optimizer = tf.compat.v1.train.GradientDescentOptimizer(learning_rate=learning_rate)\n",
    "\n",
    "train_step_counter = tf.compat.v2.Variable(0)\n",
    "\n",
    "tf_agent = reinforce_agent.ReinforceAgent(\n",
    "    train_env.time_step_spec(),\n",
    "    train_env.action_spec(),\n",
    "    actor_network=actor_net,\n",
    "    optimizer=optimizer,\n",
    "    normalize_returns=True,\n",
    "    train_step_counter=train_step_counter)\n",
    "tf_agent.initialize()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 286,
   "metadata": {},
   "outputs": [],
   "source": [
    "eval_policy = tf_agent.policy\n",
    "collect_policy = tf_agent.collect_policy"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 287,
   "metadata": {},
   "outputs": [],
   "source": [
    "def compute_avg_return(environment, policy, num_episodes=10):\n",
    "  total_return = 0.0\n",
    "  for _ in range(num_episodes):\n",
    "\n",
    "    time_step = environment.reset()\n",
    "    episode_return = 0.0\n",
    "\n",
    "    while not time_step.is_last():\n",
    "      action_step = policy.action(time_step)\n",
    "      time_step = environment.step(action_step.action)\n",
    "      episode_return += time_step.reward\n",
    "    total_return += episode_return\n",
    "\n",
    "  avg_return = total_return / num_episodes\n",
    "  return avg_return.numpy()[0]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 288,
   "metadata": {},
   "outputs": [],
   "source": [
    "def collect_episode(environment, policy, num_episodes):\n",
    "\n",
    "  episode_counter = 0\n",
    "  environment.reset()\n",
    "\n",
    "  while episode_counter < num_episodes:\n",
    "    time_step = environment.current_time_step()\n",
    "    action_step = policy.action(time_step)\n",
    "    next_time_step = environment.step(action_step.action)\n",
    "    traj = trajectory.from_transition(time_step, action_step, next_time_step)\n",
    "\n",
    "    # Add trajectory to the replay buffer\n",
    "    replay_buffer.add_batch(traj)\n",
    "\n",
    "    if traj.is_boundary():\n",
    "      episode_counter += 1"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 289,
   "metadata": {},
   "outputs": [],
   "source": [
    "replay_buffer = tf_uniform_replay_buffer.TFUniformReplayBuffer(\n",
    "    data_spec=tf_agent.collect_data_spec,\n",
    "    batch_size=train_env.batch_size,\n",
    "    max_length=replay_buffer_capacity)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "step = 10: Average Return = 1.0 Train return = 7.900000095367432\n",
      "step = 20: Average Return = 2.0 Train return = 7.5\n",
      "step = 25: loss = -0.2913815379142761\n",
      "step = 30: Average Return = 3.0 Train return = 8.0\n",
      "step = 40: Average Return = 3.0 Train return = 7.800000190734863\n",
      "step = 50: loss = -0.34632986783981323\n",
      "step = 50: Average Return = 4.0 Train return = 7.599999904632568\n",
      "step = 60: Average Return = 5.0 Train return = 8.199999809265137\n",
      "step = 70: Average Return = 5.0 Train return = 8.399999618530273\n",
      "step = 75: loss = -0.38549041748046875\n",
      "step = 80: Average Return = 6.0 Train return = 8.300000190734863\n",
      "step = 90: Average Return = 6.0 Train return = 7.599999904632568\n",
      "step = 100: loss = -0.5429959297180176\n",
      "step = 100: Average Return = 7.0 Train return = 9.0\n",
      "step = 110: Average Return = 6.0 Train return = 8.199999809265137\n",
      "step = 120: Average Return = 6.0 Train return = 8.0\n",
      "step = 125: loss = -0.37788811326026917\n",
      "step = 130: Average Return = 6.0 Train return = 8.5\n",
      "step = 140: Average Return = 6.0 Train return = 8.800000190734863\n",
      "step = 150: loss = -0.33755674958229065\n",
      "step = 150: Average Return = 6.0 Train return = 8.100000381469727\n",
      "step = 160: Average Return = 6.0 Train return = 8.0\n",
      "step = 170: Average Return = 6.0 Train return = 7.800000190734863\n",
      "step = 175: loss = -0.6063610315322876\n",
      "step = 180: Average Return = 6.0 Train return = 8.100000381469727\n",
      "step = 190: Average Return = 6.0 Train return = 8.5\n",
      "step = 200: loss = -0.3944908678531647\n",
      "step = 200: Average Return = 6.0 Train return = 7.800000190734863\n",
      "step = 210: Average Return = 6.0 Train return = 8.100000381469727\n",
      "step = 220: Average Return = 7.0 Train return = 8.199999809265137\n",
      "step = 225: loss = -0.3207050859928131\n",
      "step = 230: Average Return = 6.0 Train return = 8.399999618530273\n",
      "step = 240: Average Return = 7.0 Train return = 8.399999618530273\n",
      "step = 250: loss = -0.5809363126754761\n",
      "step = 250: Average Return = 7.0 Train return = 7.5\n",
      "step = 260: Average Return = 8.0 Train return = 8.600000381469727\n",
      "step = 270: Average Return = 8.0 Train return = 8.100000381469727\n",
      "step = 275: loss = -0.4184759259223938\n",
      "step = 280: Average Return = 7.0 Train return = 8.5\n",
      "step = 290: Average Return = 8.0 Train return = 8.199999809265137\n",
      "step = 300: loss = -0.3849588930606842\n",
      "step = 300: Average Return = 9.0 Train return = 8.600000381469727\n",
      "step = 310: Average Return = 9.0 Train return = 8.5\n",
      "step = 320: Average Return = 10.0 Train return = 7.099999904632568\n",
      "step = 325: loss = -0.5418455004692078\n",
      "step = 330: Average Return = 9.0 Train return = 7.800000190734863\n",
      "step = 340: Average Return = 8.0 Train return = 8.600000381469727\n",
      "step = 350: loss = -0.3322322964668274\n",
      "step = 350: Average Return = 7.0 Train return = 8.699999809265137\n",
      "step = 360: Average Return = 7.0 Train return = 7.400000095367432\n",
      "step = 370: Average Return = 7.0 Train return = 8.199999809265137\n",
      "step = 375: loss = -0.36994075775146484\n",
      "step = 380: Average Return = 8.0 Train return = 8.199999809265137\n",
      "step = 390: Average Return = 8.0 Train return = 7.900000095367432\n",
      "step = 400: loss = -0.43873119354248047\n",
      "step = 400: Average Return = 7.0 Train return = 8.199999809265137\n",
      "step = 410: Average Return = 7.0 Train return = 8.0\n",
      "step = 420: Average Return = 8.0 Train return = 7.699999809265137\n",
      "step = 425: loss = -0.5682023167610168\n",
      "step = 430: Average Return = 8.0 Train return = 8.399999618530273\n",
      "step = 440: Average Return = 9.0 Train return = 8.100000381469727\n",
      "step = 450: loss = -0.39259281754493713\n",
      "step = 450: Average Return = 9.0 Train return = 7.400000095367432\n",
      "step = 460: Average Return = 8.0 Train return = 8.0\n",
      "step = 470: Average Return = 10.0 Train return = 8.699999809265137\n",
      "step = 475: loss = -0.36335477232933044\n",
      "step = 480: Average Return = 10.0 Train return = 8.199999809265137\n",
      "step = 490: Average Return = 10.0 Train return = 8.5\n",
      "step = 500: loss = -0.5142598152160645\n",
      "step = 500: Average Return = 10.0 Train return = 8.199999809265137\n"
     ]
    }
   ],
   "source": [
    "# (Optional) Optimize by wrapping some of the code in a graph using TF function.\n",
    "tf_agent.train = common.function(tf_agent.train)\n",
    "\n",
    "# Reset the train step\n",
    "tf_agent.train_step_counter.assign(0)\n",
    "\n",
    "# Evaluate the agent's policy once before training.\n",
    "avg_return = compute_avg_return(eval_env, tf_agent.policy, num_eval_episodes)\n",
    "train_avg_return = compute_avg_return(eval_env, tf_agent.collect_policy, num_eval_episodes)\n",
    "returns = [avg_return]\n",
    "train_returns = [train_avg_return]\n",
    "\n",
    "for _ in range(num_iterations):\n",
    "\n",
    "  # Collect a few episodes using collect_policy and save to the replay buffer.\n",
    "  collect_episode(\n",
    "      train_env, tf_agent.collect_policy, collect_episodes_per_iteration)\n",
    "\n",
    "  # Use data from the buffer and update the agent's network.\n",
    "  experience = replay_buffer.gather_all()\n",
    "  train_loss = tf_agent.train(experience)\n",
    "  replay_buffer.clear()\n",
    "\n",
    "  step = tf_agent.train_step_counter.numpy()\n",
    "\n",
    "  if step % log_interval == 0:\n",
    "    print('step = {0}: loss = {1}'.format(step, train_loss.loss))\n",
    "\n",
    "  if step % eval_interval == 0:\n",
    "    avg_return = compute_avg_return(eval_env, tf_agent.policy, num_eval_episodes)\n",
    "    train_avg_return = compute_avg_return(eval_env, tf_agent.collect_policy, num_eval_episodes)\n",
    "    print('step = {0}: Average Return = {1} Train return = {2}'.format(step, avg_return,\n",
    "                                                                      train_avg_return))\n",
    "    returns.append(avg_return)\n",
    "    train_returns.append(train_avg_return)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "Text(0.5, 0, 'Step')"
      ]
     },
     "execution_count": 17,
     "metadata": {},
     "output_type": "execute_result"
    },
    {
     "data": {
      "image/png": "iVBORw0KGgoAAAANSUhEUgAAAX4AAAEGCAYAAABiq/5QAAAABHNCSVQICAgIfAhkiAAAAAlwSFlzAAALEgAACxIB0t1+/AAAADh0RVh0U29mdHdhcmUAbWF0cGxvdGxpYiB2ZXJzaW9uMy4xLjMsIGh0dHA6Ly9tYXRwbG90bGliLm9yZy+AADFEAAAgAElEQVR4nO3deXhb133m8e+PuyhSK0lI1kbtIr3JjmzLmyxLosdZ6rRNMo2TtkmbVs00aZyk0zZp0+15On3azkyWTqeduk0m0yZNHMdNnThpKlCLd8uWbNmWQG2UJWsjSIoS9x1n/gAgUxJIgiQW4t738zx8SFxc4p4L0z8dnHvue8w5h4iI+EdethsgIiKZpcIvIuIzKvwiIj6jwi8i4jMq/CIiPlOQ7QYko6KiwlVXV2e7GSIiOWX//v2tzrnKq7fnROGvrq5m37592W6GiEhOMbNTibZrqEdExGdU+EVEfEaFX0TEZ1T4RUR8RoVfRMRn0lb4zewbZtZsZgdHbJtnZkEzOxb7PjddxxcRkcTS2eP/JvDgVdu+AOx0zq0GdsYei4hIBqVtHr9z7hkzq75q8/uBzbGf/x+wB/i9dLVBRKJ2NoRZXjGTFZVl2W6K5x0Ld/Kj18+l7PU+dlc188uKU/Z6kPkbuALOufMAzrnzZlY12o5mth3YDrB06dIMNU/Eezr6Bvnkt/azaXUlX//4bdlujud9becxnnrjPGapeb2H1i/K+cKfNOfco8CjABs2bNBqMSKT9PSRFgaHHc8db6VnYIjSomn7v70nNLZ0s3ltJd/8lduz3ZRRZXpWT9jMFgLEvjdn+PgivhMMhckz6B+K8Oyx1mw3x9MiEcdbrV2snOZDapku/D8EPhb7+WPAkxk+voivDA5H2H2kmZ9dv4hZJQUEQ+FsN8nTzrX30jcYYUXlzGw3ZUxp+8xnZt8heiG3wszOAH8M/AXwPTP7BPA28KF0HV9EYO+JNjr7hnjwhgVEnGPX4WaGI478vBQNQMsVTrR0A7CiYnr3+NM5q+fhUZ7amq5jisiV6hvCFBfkce/qSgaGI/zbgXPsP3WR25fPy3bTPOlESxcAK6umd49fd+6KeJRzjmAozL2rK5hRlM99ayopzDeCoaZsN82zGlu6KS8uoDLFs3BSTYVfxKNC5zs4e6mXutoAAOUlhdy5soJgKIxzmiiXDidau1hRVYalai5nmqjwi3hUMBTGDLasC1zeVlcb4OSFHo43d2WxZd7V2NzNyorpPcwDKvwinhUMhbl16Vwqy98Zdqirif4jEGzQ7J5U6+ofoqmjj5VV0/vCLqjwi3jSuUu9HDrXcXmYJ27B7BJuWjxb0zrT4K3LM3rU4xeRLKiP9eivLvwA22oCHDh9iebOvkw3y9NOtEaHz3IhD0mFX8SDgqEwKypnJryDtK42gHOws0E3zqdSY0s3eQbL5pdmuynjUuEX8ZiOvkFeOnHh8nj+1dYtKGfx3Bka7kmxxpYuFs8tpaQwP9tNGZcKv4jH7ImFsiUa5gEwM+pqAzx3vJXu/qEMt867TrR0s3KaRzXEqfCLeEx9KMz8mUXcsnT0Be7qagMMDEV49lhLBlvmXfFwtlwY3wcVfhFPiYeyba2pGjOP57bqecwqKWCHhntSIlfC2eJU+EU8JB7KVle7YMz9CvPz2LKuil2HmxkajmSodd4VD2eb7nHMcSr8Ih4SDDVRUpjHPasqxt23rnYBl3oG2X/qYgZa5m2NLfGpnLnR49dSPCI5ZmAowu98/3VaOvuvee7NM+3cs6qSGUXjzyy5b20lRfl5BENh7lgxPx1NzSlfCR7llZNtCZ/7mZuv4+HbR18C9kRLN+Ul0z+cLU49fpEc8+KJCzx54BztvYMMDkeu+Kq5bhafuGd5Uq9TVlzAnSvnE2xQaNulngH+Zvdxzl7qveY9bWzp4svBo0Qio79HjS3RC7vTPZwtTj1+kRwTDDUxozCfJ/7LXVOeM15XG+BL/3aQY81drAmUp6iFuSe+QM3XPnwL65fMueK5H7x2hs899jqvn7k06kypEy3d3LUydz41qccvkkOcc9SHmtm0piIlNwpti4e2+Xx2TzAUpqq8mJsWzb7mufvXRmdIjfYe5VI4W5wKv0gOOXi2g6aOvnFn7SRLoW3QNzjM00db2FYbIC/BFNg5pUXcXj3vcv7R1XIpnC1OhV8khwRDTeQZbFlXlbLXrIuHtnX4M7TtxRMX6BkYHvVOZ4gOiR0Nd3HqQvc1z8XD2dTjF5G02BEKs6F6HvNmFqXsNeuujxa8ep+GtgVDYWYW5Y85Rh//RyHRJ6PG5q6cCWeLU+EXyRGn23o43NTJA2P0TCdjbaCcJfNm+HIt3kjEUR8Kc9/aSooLRr9msmReKesWlCe807mxtZvFc0vH/P3pRoVfJEfEe5vbRkndnCwzo65mAc83XvBdaNsbZ9tp7uxP6j2tqw2w72Qbbd0DV2xvbO7KmXC2OBV+kRwRDIVZXVVGdRouIm6rrfJlaFsw1ER+niV1zaSuNkDEwe7D7wyJRSKOkxe6cyacLU6FXyQHXOoZ4OWTbWNegJyK26vnMXtGoe9C24KhMLdVz2VO6fjXTG5cNJvArOIrxvnj4Wy5ktETp8IvkgP2HGlhODJ6xv5UFfgwtO3UhW6OhruSnhprZmyrCfDMsRb6BoeB6KpbkDsZPXEq/CI5IH6D0c2L54y/8yTV1Qa41DPIPp+EtsV77hO5WF5XG6BnYJgXGlsBOBELZ1OPX0RSqn9omD1Hmtlak/gGo1TZtOad0DY/CIbCrFtQzpJ5yU/DvHPlfMqKCy6/R40tXZSXFFBRlrrptZmgwi8yzb3YeIHugeGUT+O8WllxAXetmk8w5P3QtovdA7wyiWsmxQX53LemkvqGZiIRx4mW7pwKZ4tT4ReZ5oKhMKVF+dyZgRCwbTUB3m7r4VhzV9qPlU27DjcTcZObGltXG6Cls5/Xz1zKqXV2R1LhF5nGIhFHfUOYTasrUxLKNp6x7lD1kmAoTGBWMTcmCGUbz+a1leTnGU8eOBcNZ8ux8X1Q4ReZ1g6eayfc0Z+22TxXC8wq4ebFsz09rbNvcJhnjrWwbZLXTOKhbY+9chpAPf5kmdnnzOyQmR00s++YWUk22iEy3QVD4aRvMEqVutoAr5++RNijoW0vNo4fyjaeutoAvbEpnbl28xZkofCb2SLgM8AG59wNQD7w4Uy3QyQXBENhNiyby9wUhrKNJz6vfbQY4ly3IxS+vPrYZMX/0ci1cLa4bK3AVQDMMLNBoBQ4l6V2iKTcTw828bnHDjCcYKm+2aWF/Pgz91BVPv6H3Hgo25feW5OOZo5qTaCMpfNKqQ+F+egdy9JyjGeOtvDJb+1naPja96ispIAffvpuFs+dfEHtGxzmwa8+w7lL135qGRiO8N4bF04pVC0e2tY7OJxT4WxxGS/8zrmzZvY/gLeBXmCHc27H1fuZ2XZgO8DSpaMvciwy3Ty+7zRlJQV88F2Lr9jeOzDMN184yY5DYX5x4/gFNT7Onqnx/bj4Harf2nuK7v4hZhanvkx8f/8Zigry+NhdV/6/PTQc4R+efYufvHme7ZtWTvr1nzvWyskLPXzoXYupKL9yAXQDfv7WxYl/cQL+4gM30TOQm6F2GS/8ZjYXeD+wHLgEPG5mv+ic+9bI/ZxzjwKPAmzYsMHbk4rFM3oGhnjueCsP376U33tw3RXPOefYc6SZYCi5wh8MNbEmUMay+Zm/eFhXG+Abz7/FM0dbePeNC1P62oPDEXYfaebdNyy45j0CeP74BYKh8JQKfzAUpry4gP/2czdSVJCeEe2r1+bNJdm4uLsNeMs51+KcGwT+FbgrC+0QSblnjrbSPxRJeLOVmVFXG+DFxgt0jRN/fKlngFdOXsx4bz8uGlxWmJZpnXtPtNHZNzRqRk5dbYD9py5yoat/Uq8/HHHsPBxm87qqtBX9XJeNd+VtYKOZlVr0dretQEMW2iGScvUNYWaVFHDb8nkJn99WE2BgOMLTR8aOP959pDkWypaatXUnqiA/jy1rq9h1JPWhbcFQEyWFedyzqiLh8/H4412HJ7ci2IHTF2ntGmBbTeZmQuWajBd+59xe4PvAq8CbsTY8mul2iKTacMSx63Az96+rojA/8f9a71o2l7mlheOudhUPZbtpEjcYpUo8tO2Vk6kLbXPOEQyFuWdVJTOKEl8Uvf66WVw3u2TSnzZ2hMIU5Bmb16rwjyYrn4Occ3/snFvnnLvBOfdLzrnJfaYTmUb2n7pIW/fAmMMz0fjjALsONzM4Sk+6f2iYp4+0sK02vaFs49m0ppKigtSGtoXOd3CuvW/M3CEzY1ttgGePtV6OP56I+lCYjSvmM3tG4VSa6mkaABNJkWCoicJ84741lWPuV1cboKNviFdOtiV8/oVYKFtdipdYnKiZxQXcvXI+wYamlIW2BUNhzGDLOMMw8RuknjvWOqHXP9HSRWNLd9aujeQKFX6RFIgPYdy5soLykrF7mpvWVIzZk67PYCjbeOpqF3C6rZej4dSEtgVDYd61dC4VZcVj7nfH8vmUj4g/nsjrA2xT4R+TCr9ICjS2dHHyQk9SPc3SogLuWVWRMP44Hsp235rMhLKNZ2usZz7eNYlknL3Uy6FzHUm9R0UFedy3tpKdh8NEEtwIN5pgKEztwlksmjNjKk31PBV+kRSI32yV7EySutoAZy72crip84rtb57NbCjbeAKzSrh5yZyUjPPXT/CGtLraAK1dA7x2+lJS+7d29bP/7exNgc0lKvwiKRAMhblx0WwWzk6up7m1pgqza+OPsxHKNp4HagO8fqZ9yqFtwVCYFZUzkw4127y2ioI8S/ofnV0NzTiX+Tudc5EKv8gUNXf2ceD0pQkVnKryEtYn6EkHQ+HYzVPTZym/VGT0t/cO8tKJCxN6j2bPKGTjivlJDzPtCIVZNGcG1183a7LN9A0VfpEp2jnJnmZdbYA3z7Zzvr0XgLcv9HAk3DmpVaHSaXVVGcvml04prfPpoy0MRdyEl4+sqw3Q2NJ9eVHz0fQODPPc8Ra21VTl3DKI2aDCLzJFwVCYxXNnsG5B+YR+Lz5ds74heodqMFZYH8jS3bqjMTPqagK8cHz8qInRBENhKsqKWL9k7oR+752Ly2P/o/Pc8Vb6BiOazZMkFX6RKYiHstXVBibc01xVVUb1/NLLRS0YamJtoJyl0zDfva42GjXxzNGxoyYSGRiKsOdwM1vXBcif4A1pi+eWUrtw1rifNoKhJsqLC7hjefanwOYCFX6RKXjmaCsDQ5FJXVB8J7StldNtPVkNZRvPO1ETEx/u2fvWBTr7hyZ9buOFtg1HHDsbmhXKNgF6l0SmIBgKM3tGIbdVJw5lG09d7QIGhx1/+qNDsVC26Vn4C/LzuH9d1ZhRE6MJhsLRULbViUPZxhMPbds5Smjba29f5MI4URlyJRV+kUkaGo6w63CY+9dWjhrKNp54T7q+oZnArGJuzGIo23geqA3Q3js4atREIs456kNh7l09+RvSxgttC14OZRs7KkPeka2lFyVN2nsHKcrPGzX5UKCjb5B8s6RXluobHOZouPOa7UfDXVzsGZxSdHJ0zn6AJ149w9aa7Iayjefe1dHQtif2n6UsyffudFsv59r7+GzdmkkfNx7a9r19p3n17YsUXPUe7YiFss0aJypD3qHC7yHOOT70f15gTaCcv/nIrdluzrT1y19/mXkzi/jGx29Lav8/evIg39t3JuFzxbFogal49w0LeOLVM7z7huk1m+dqM4sL2LS6kidePcMTryZ+PxIpyDO2TvGGtAevX8A/vXiKn//bFxI+/6v3LJ/S6/uNCr+HHGvu4mi4i7fbeugdGFavP4HTbT0cOH2JgjyjvXdw3OjeweEI/3EoOpyTaLnERXNnJN37Hc3Wmiqe+q17uGEaD/PE/eUHbuTA6SUT+p3ArBLmjxPKNp47V87nu9s30p1gOmlhft60CLTLJSr8HhIfA+0bjPD88VbNaU5gZ2xa4FAkuv7t+9cvGnP/fScv0t47yC/ctoStabqxysxyougDzC8rTtv7MBYzY+MKFfdUGfeKlJlVmtnvm9mjZvaN+FcmGicTE08mnEycrV8EG6J5MRVlxUm9R8FQmKKCPO5drQuH4h3J9PifBJ4F6oGJL4cjGdHcEc2L+a8PrOFwUyc7D4cZjrgJ3zDjZe29g+w90cav3buCSz0D/PiN8wwMRUad++2cI9jQxD2rKpK+ECySC5KZg1bqnPs959z3nHNPxL/S3jKZkPht/3W1Cy7H2R44nbq1Ur1gz5FmhmJz5etqA3T2D/HSiQuj7n8k3Mnptl7NDxfPSabwP2Vm70l7S2RKgqEmlsybwZpA2eU42x0a7rnCjlhezC1L5nD3qgpmFOaPOdwTPBR9bqozUkSmm2QK/yNEi3+vmXWYWaeZdaS7YZK87v4hnm+8QF3NAsyM2TMKuWPFPI3zjxBfwHzruuhc+ZLCfO5dXUF9w7WrYMUFG8KsXzKHqlklGW6tSHqNWfgtmjp1vXMuzzk3wzk3yzlX7pxT4PU08szRlmvyYupqApxo6aZxnDhbv3jpRBtdV+XF1NUGON/ex6Fz1/Zjmtr7eONMu4Z5xJPGLPwu2hX6QYbaIpMUbAgzp7SQ26rfibyNT+WsV68fiL4PMwrzr8iL2VoTIM9IOCRWfzkiWYVfvCeZoZ6XzCy5Wxwl46J5Mc1sWVtFwYi8mHicrYZ7YnkxDWHuXV1xRV7MvJlFbFiWeEgsGApTPb+UVVXJLRMokkuSKfz3Ay+aWaOZvWFmb5rZG+lumCRn36mLXOoZTHizVl1tgP1vX6R1lDhbvzh4toPz7X0Jh2221VbRcL6D0209l7d19Q/xYuOFSWXsi+SCZAr/u4GVwBbgZ4D3xb7LNBAMhSnKz2PTmmtvMKqrDeBcdBFqPwuGmsgzEi5gHg9YG7nQx9NHWhgYjky7JRBFUiWZwu9G+ZIsc84RDIW5a9X8hHkx8Thbv0/r3BEK865lcxPmxSyvmMmqqrIrhnuCoSbmlhbyrmUTWyZQJFckU/h/DDwV+74TOAH8ezobJcmJB7KNNvMkHmf73PEWegf8edP16bYeDjd1jjk7p642wN632mjvGWQwfs1kXeCKayYiXjLuX7Zz7kbn3E2x76uB24Hn0t80GU8w1AQw5pBEXW2AvsEIzx6b+FqpXhDvyY+Vmb+tJsBwxLHnaDOvnGyjo2/yywSK5IIJd2mcc68CmuUzDQQbmrl58WwCY9xgdMfy+ZQXF4y7WLVX1TeEWVVVxvKKmaPuc8uSOVSUFbMjFCYYClNckMemNZNbJlAkF4ybPGVmnx/xMA+4FZhS99HM5gD/CNxA9HrBrzrnXpzKa/pNuKOP12OhbGMpKshj87oqdjY0+y60rb1nkL1vtbF904ox98vLM7bVVPHUG+eZPaOQe1ZVUFqkUDbxrmR6/OUjvoqJjvW/f4rH/RrwU+fcOuBmoGGKr+c78R58Msv+1dUGuNA9wGtv+yu0bfeR5qQXMK+rDdDVP8TZSwplE+9LplsTcs49PnKDmX0IeHyU/cdkZrOATcDHAZxzA8DAZF7LD4KhMG+PmGMe94PXzrB0XilrAuPfYHTfmkoK8oy/3dPI3ava09HMaenHb5yjoqyY9YvnjLtvPLStb2iYLTUKZRNvS6bwf5Fri3yibclaQXSo6P+a2c3AfuAR51z3yJ3MbDuwHWDp0qWTPFRuO9/ey6//075Rn//MllVJ3WA0e0Yh22oC/PRQE7sO+2tO/yfuWZ7UAuYlhfm876aFNHf2U1WuUDbxNhstmdDM3g28B/jPwGMjnpoF1Drnbp/UAc02AC8Bdzvn9prZ14AO59wfjvY7GzZscPv2jV4AveqfXzzJHz55iCc/dTfVV12cNIPy4oKk7yyNRBydCdYr9bpZJcm/RyJeY2b7nXMbrt4+Vo//HLAPeIhorzyuE/jcFNpyBjjjnNsbe/x94AtTeD3P2hHLi7lp8ewpF6+8PBt3YXER8YdRC79z7nXgdTP7l9h+S51zR6Z6QOdck5mdNrO1sdfbCoSm+rpe09k3yEsnLvDxu6rVYxWRlEpmVs+DwAHgpwBmtt7MfjjF4/4W8O1Y2Nt64M+n+Hqe8/TRFgaHXVKzdkREJiKZi7t/QvRu3T0AzrkDZlY9lYM65w4A14w7yTuCoTDzZhYpL0ZEUi6ZHv+Qc84/cwCngcHhCLsPN7NlXZWvbrgSkcxIpsd/0Mw+AuSb2WrgM8AL6W2Wv738lvJiRCR9kunx/xZwPdAP/AvQAXw2nY3yu3hezL2rlRcjIqk3bo/fOdcD/EHsCwAzWwacSmO7fCuesa+8GBFJlzF7/GZ2p5l90MyqYo9vik3vVCxzmjSc71RejIik1aiF38z+O/AN4APAj83sj4EgsBdYnZnm+U8wFMYMtmrZPxFJk7HGEt4L3OKc6zOzuUTv5L3JOXcsM03zp2BDE7csmUNl+bXLBIqIpMJYQz29zrk+AOfcReCIin56nbvUy8GzHbppS0TSaqwe/8qr7tCtHvnYOfdQ+prlTzsvZ+xrmEdE0meswn/1Yiv/M50NkWgo24qKmayqGj9jX0RkssYKaXs6kw3xu45YKNuv3L08200REY+b8GLrkh5PH4mHsmmYR0TSS4V/mgiGwsyfWcStSxXKJiLplfStoWY28+rlEWViegaG+JMfHqKz79qVsJ4+2sJ7b1yoUDYRSbtxC7+Z3QX8I1AGLI2tk/sbzrnfTHfjvCYYCvO9fWdYUTGTgvwrC3z1/Jk8fIc/1xYWkcxKpsf/FeA/AT+E6MpcZrYpra3yqGAoTEVZMfWfvy+pBcBFRNIhqTF+59zpqzYNp6EtnjYwFOHpIy1sq6lS0ReRrEqmx386NtzjzKyIaB5/Q3qb5T0vnbhAZ78y9kUk+5Lp8X8S+BSwCDhDdI3cT6WzUV4UDIWZUZjP3auUsS8i2ZVMHn8r8NEMtMWznHPUN4S5d3UFJYX52W6OiPhcMrN6/jrB5nZgn3PuydQ3yXsOnu3gfHsfn69bk+2miIgkNdRTQnR451js6yZgHvAJM/tqGtvmGcFQE3nK2BeRaSKZi7urgC3OuSEAM/s7YAdQB7yZxrZ5RrChmQ3L5jFvZlG2myIiklSPfxEwc8TjmcB1zrlhoguwyxhOt/XQcL5Ds3lEZNpIpsf/V8ABM9sDGLAJ+HMzmwnUp7FtnlCvjH0RmWaSmdXzdTP7CXA70cL/+865c7GnfyedjfOCYCjM6qoyqitmjr+ziEgGJJvO2QecB9qAVYpsSE57zyB732pjm3r7IjKNJDOd89eAR4DFwAFgI/AisCW9Tct9u480MxxRxr6ITC/J9PgfAW4DTjnn7gduAVrS2iqPCIbCVJYXs37xnGw3RUTksmQKf59zrg/AzIqdc4eBteltVu7rHxpmz5FmhbKJyLSTzKyeM2Y2B/g3IGhmF4Fz4/yO7710oo3ugWEN84jItJPMrJ6fi/34J2a2G5gN/HSqBzazfGAfcNY5976pvt50Eww1UVqUz10rFcomItPLmIXfzPKAN5xzNwA4555O4bEfIRrvPCuFrzktOOeoDzWzaXWlQtlEZNoZc4zfORcBXjezlK4JaGaLgfcSXdLRc948205TR5+mcYrItJTMGP9C4JCZvQxcXmzdOffQFI77VeB3gfLRdjCz7cB2gKVLc2st2mAoTJ7BlnVV2W6KiMg1kin8f5rKA5rZ+4Bm59x+M9s82n7OuUeBRwE2bNjgUtmGdAuGwmyoViibiExP407njI3rnwQKYz+/Arw6hWPeDTxkZieB7wJbzOxbU3i9aeV0Ww+Hmzp5QMM8IjJNjVv4zezXge8Dfx/btIjo1M5Jcc590Tm32DlXDXwY2OWc+8XJvt50EwwplE1EprdkbuD6FNFeegeAc+4YoMHrUcRD2ZbNVyibiExPyRT+fufcQPyBmRUAKRlzd87t8dIc/ks9A7x8sk29fRGZ1pIp/E+b2e8DM8ysDngc+FF6m5WbFMomIrkgmcL/BaKhbG8CvwH8BPhSOhuVq+pDzVSVF3OzQtlEZBpLZjrn+4F/cs79Q7obk8vioWwPrV+kUDYRmdaS6fE/BBw1s382s/fGxvjlKi82XqB7YFjTOEVk2ktmHv+vAKuIju1/BGg0M09GLUxFMBSmtCifO1fOz3ZTRETGlFTv3Tk3aGb/TnQ2zwyiwz+/ls6G5ZJIxFHfEFYom4jkhGRu4HrQzL4JHAc+SDRYbWGa25VTDp5rJ9zRr9k8IpITkunxf5xotMJvOOf609uc3BQMhcnPM4WyiUhOSGYhlg+PfGxmdwMfcc59Km2tyjHBUJgNy+YyV6FsIpIDkpnVg5mtN7O/igWr/RlwOK2tyiHxUDYN84hIrhi1x29ma4iGqD0MXAAeA8w5d3+G2pYTdiiUTURyzFhDPYeBZ4Gfcc4dBzCzz2WkVTmkPhRmTUChbCKSO8Yq/B8g2uPfbWY/JXqB15e3pDrnOHOxl4i7MpuuZ2CYl0+28cn7VmSpZSIiEzdq4XfO/QD4gZnNBH4W+BwQMLO/A37gnNuRoTZm3bdeOsUfPnlo1OcfqF2QwdaIiExNMrN6uoFvA982s3nAh4gGt/mm8P/ojfOsqJjJp7esuua5uaVF3LxEoWwikjsmlLvjnGsjuhLX34+3r1e0dQ+w72Qbn75/FT9/6+JsN0dEZMqSms7pZ7sONxNxUKfhHBHxCBX+cQRDTSyYVcINi2ZluykiIimhwj+GvsFhnjnayrbaKsx8OaFJRDxIhX8Mzx9vpXdwWMM8IuIpKvxjqG8IU1ZcwMYV87LdFBGRlFHhH0U0Y7+Z+9ZWUlygjH0R8Q4V/lEcOHOJls5+LaUoIp6jwj+KYChMQZ6xeY0y9kXEW1T4RxEMhbl9+TxmlxZmuykiIimlwp/AW63dHG/uUtSyiHiSCn8CwVAToIx9EfEmFf4E6kPN1CycxeK5pdluiohIyqnwX6Wte4B9p9rU2xcRz1Lhv8rOhjARh6ZxiohnZbzwm9kSM9ttZg1mdsjMHsl0G8YSDIVZOLuE669TKJuIeFM2evxDwG8752qAjcCnzKw2C+24RoVIgcAAAAkNSURBVN/gMM8ea2VbTUChbCLiWRkv/M658865V2M/dwINwKJMtyOR547FQ9k0zCMi3pXVMX4zqwZuAfYmeG67me0zs30tLS0ZaU8wFKa8uICNK+Zn5HgiItmQtcJvZmXAE8BnnXMdVz/vnHvUObfBObehsrIy7e2JRBw7D4e5b20lRQW65i0i3pWVCmdmhUSL/redc/+ajTZc7bXTl2jtGtAwj4h4XjZm9RjwdaDBOfflTB9/NJdD2dYqlE1EvC0bPf67gV8CtpjZgdjXe7LQjisEQ03csWIes2colE1EvK0g0wd0zj0HTKu5kidaumhs6eaXNi7LdlNERNJOVzGJDvMAbNP4voj4gAo/0bV1axXKJiI+4fvCf6Grn/2nLmo2j4j4hu8L/87DzUScsvdFxD98X/iDoTDXKZRNRHzE14W/d2CYZ4+1sK1WoWwi4h++LvzPH2+lbzCiYR4R8RVfF/54KNsdyxXKJiL+4dvCP6xQNhHxKd9WvAOnLyqUTUR8ybeFf4dC2UTEp3xb+OtDYTaumK9QNhHxHV8W/ngom4Z5RMSPfFn446FsW2s0zCMi/uPbwq9QNhHxK98V/taufva/rVA2EfEv3xX+XQ3NOIWyiYiP+a7wBxvCLJozQ6FsIuJbvir8l0PZaqoUyiYivuWrwv9cLJRNSyyKiJ/5qvAHQ00KZRMR3/NN4R+OOHY2NLN5XZVC2UTE13xTAV97+yIXuhXKJiLim8IfbAhTmG9sXluZ7aaIiGSVfwp/LJRtVolC2UTE33xR+BtbujjR0s22Gg3ziIj4ovDHQ9k0jVNExEeF//rrZrFozoxsN0VEJOs8X/hbOvt5VaFsIiKXeb7w7z6sUDYRkZE8X/h3hKKhbLULFcomIgJZKvxm9qCZHTGz42b2hXQdp3dgmOeOt1BXG1Aom4hITMYLv5nlA/8beDdQCzxsZrXpONazx1qioWyaxikiclk2evy3A8edcyeccwPAd4H3p+NAwVCY8pIC7lgxLx0vLyKSk7JR+BcBp0c8PhPbdgUz225m+8xsX0tLy6QOtLxyJh+9YxmF+Z6/lCEikrSCLBwz0WC7u2aDc48CjwJs2LDhmueT8ZubV03m10REPC0bXeEzwJIRjxcD57LQDhERX8pG4X8FWG1my82sCPgw8MMstENExJcyPtTjnBsys08D/wHkA99wzh3KdDtERPwqG2P8OOd+AvwkG8cWEfE7TXcREfEZFX4REZ9R4RcR8RkVfhERnzHnJnVvVEaZWQtwapK/XgG0prA5uUDn7A86Z++b6vkuc85VXr0xJwr/VJjZPufchmy3I5N0zv6gc/a+dJ2vhnpERHxGhV9ExGf8UPgfzXYDskDn7A86Z+9Ly/l6foxfRESu5Icev4iIjKDCLyLiM54u/Jla1D3TzOwbZtZsZgdHbJtnZkEzOxb7Pje23czsr2PvwRtmdmv2Wj45ZrbEzHabWYOZHTKzR2LbvXzOJWb2spm9HjvnP41tX25me2Pn/Fgs2hwzK449Ph57vjqb7Z8KM8s3s9fM7KnYY0+fs5mdNLM3zeyAme2LbUvr37ZnC38mF3XPgm8CD1617QvATufcamBn7DFEz3917Gs78HcZamMqDQG/7ZyrATYCn4r9t/TyOfcDW5xzNwPrgQfNbCPwl8BXYud8EfhEbP9PABedc6uAr8T2y1WPAA0jHvvhnO93zq0fMWc/vX/bzjlPfgF3Av8x4vEXgS9mu10pPL9q4OCIx0eAhbGfFwJHYj//PfBwov1y9Qt4EqjzyzkDpcCrwB1E7+IsiG2//DdOdH2LO2M/F8T2s2y3fRLnujhW6LYATxFdqtXr53wSqLhqW1r/tj3b4yfJRd09JOCcOw8Q+14V2+6p9yH2cf4WYC8eP+fYkMcBoBkIAo3AJefcUGyXked1+Zxjz7cD8zPb4pT4KvC7QCT2eD7eP2cH7DCz/Wa2PbYtrX/bWVmIJUOSWtTdBzzzPphZGfAE8FnnXIdZolOL7ppgW86ds3NuGFhvZnOAHwA1iXaLfc/5czaz9wHNzrn9ZrY5vjnBrp4555i7nXPnzKwKCJrZ4TH2Tck5e7nH77dF3cNmthAg9r05tt0T74OZFRIt+t92zv1rbLOnzznOOXcJ2EP0+sYcM4t32Eae1+Vzjj0/G2jLbEun7G7gITM7CXyX6HDPV/H2OeOcOxf73kz0H/jbSfPftpcLv98Wdf8h8LHYzx8jOg4e3/7LsdkAG4H2+EfIXGHRrv3XgQbn3JdHPOXlc66M9fQxsxnANqIXPHcDH4ztdvU5x9+LDwK7XGwQOFc4577onFvsnKsm+v/rLufcR/HwOZvZTDMrj/8MPAAcJN1/29m+sJHmiybvAY4SHRv9g2y3J4Xn9R3gPDBItAfwCaJjmzuBY7Hv82L7GtHZTY3Am8CGbLd/Eud7D9GPs28AB2Jf7/H4Od8EvBY754PAH8W2rwBeBo4DjwPFse0lscfHY8+vyPY5TPH8NwNPef2cY+f2euzrULxOpftvW5ENIiI+4+WhHhERSUCFX0TEZ1T4RUR8RoVfRMRnVPhFRHxGhV9kFGb2B7FkzDdiyYl3mNlnzaw0220TmQpN5xRJwMzuBL4MbHbO9ZtZBVAEvEB07nRrVhsoMgXq8YskthBodc71A8QK/QeB64DdZrYbwMweMLMXzexVM3s8licUz1j/y1im/stmtipbJyJyNRV+kcR2AEvM7KiZ/a2Z3eec+2uiuSj3O+fuj30K+BKwzTl3K7AP+PyI1+hwzt0O/A3RzBmRacHL6Zwik+ac6zKzdwH3AvcDj9m1q7htJLrIz/OxpNAi4MURz39nxPevpLfFIslT4RcZhYvGIu8B9pjZm7wTmhVnQNA59/BoLzHKzyJZpaEekQTMbK2ZrR6xaT1wCugEymPbXgLujo/fm1mpma0Z8Tu/MOL7yE8CIlmlHr9IYmXA/4pFIw8RTYDcDjwM/LuZnY+N838c+I6ZFcd+70tEE2EBis1sL9EO1mifCkQyTtM5RdIgtpiIpn3KtKShHhERn1GPX0TEZ9TjFxHxGRV+ERGfUeEXEfEZFX4REZ9R4RcR8Zn/D56R73A7HuCKAAAAAElFTkSuQmCC\n",
      "text/plain": [
       "<Figure size 432x288 with 1 Axes>"
      ]
     },
     "metadata": {
      "needs_background": "light"
     },
     "output_type": "display_data"
    }
   ],
   "source": [
    "steps = range(0, num_iterations + 1, eval_interval)\n",
    "plt.plot(steps, returns)\n",
    "plt.ylabel('Average Return')\n",
    "plt.xlabel('Step')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 290,
   "metadata": {},
   "outputs": [],
   "source": [
    "replay_buffer.clear()\n",
    "collect_episode(train_env, tf_agent.collect_policy, 10)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 325,
   "metadata": {},
   "outputs": [],
   "source": [
    "def encode_onehot(x, dim):\n",
    "    \"\"\"Encode x as 1-hot of dimension d.\"\"\"\n",
    "    assert x in range(dim)\n",
    "    out = np.zeros(dim)\n",
    "    out[x] = 1\n",
    "    return out\n",
    "\n",
    "def buffer_to_dataset(replay_buffer):\n",
    "    \"\"\"Create a dataset from a replay buffer.\"\"\"\n",
    "    types = replay_buffer.gather_all().step_type.numpy()[0]\n",
    "    obs = replay_buffer.gather_all().observation.numpy()[0]\n",
    "    acts = replay_buffer.gather_all().action.numpy()[0]\n",
    "\n",
    "    xs = []\n",
    "    ys = []\n",
    "\n",
    "    for t, o, a in zip(types, obs, acts):\n",
    "        oa = np.hstack([o, encode_onehot(a, v_n)])\n",
    "        if t == 0:\n",
    "            xs.append(oa)\n",
    "        elif t == 1:\n",
    "            xs.append(oa)\n",
    "            ys.append(o)\n",
    "        elif t == 2:\n",
    "            ys.append(o)\n",
    "\n",
    "    assert len(xs) == len(ys)\n",
    "    \n",
    "    return np.array(xs), np.array(ys)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 340,
   "metadata": {},
   "outputs": [],
   "source": [
    "def m_passthrough_action(decoder):\n",
    "    \"\"\"Create a model with last v_n components in input passed through.\"\"\"\n",
    "    inputs = tf.keras.Input(shape=(v_k + v_n,))\n",
    "\n",
    "    decoded_obs = decoder(inputs[:, :v_k])\n",
    "    passed_action = inputs[:, v_k:]\n",
    "\n",
    "    merged = tf.keras.layers.Concatenate()([decoded_obs,  passed_action])\n",
    "\n",
    "    model = tf.keras.Model(inputs=inputs, outputs=merged)\n",
    "    \n",
    "    return model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 350,
   "metadata": {},
   "outputs": [],
   "source": [
    "env_model = tf.keras.Sequential([\n",
    "    m_passthrough_action(decoder),\n",
    "    tf.keras.layers.InputLayer(input_shape=(v_k + v_n,)), # input: [state, one-hot action]\n",
    "    tf.keras.layers.Dense(v_k) # output: state\n",
    "])\n",
    "\n",
    "env_model.compile('adam', 'mse')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 351,
   "metadata": {},
   "outputs": [],
   "source": [
    "xs, ys = buffer_to_dataset(replay_buffer)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 353,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Train on 200 samples\n",
      "Epoch 1/500\n",
      "200/200 [==============================] - 0s 766us/sample - loss: 0.5482\n",
      "Epoch 2/500\n",
      "200/200 [==============================] - 0s 40us/sample - loss: 0.5276\n",
      "Epoch 3/500\n",
      "200/200 [==============================] - 0s 41us/sample - loss: 0.5078\n",
      "Epoch 4/500\n",
      "200/200 [==============================] - 0s 40us/sample - loss: 0.4889\n",
      "Epoch 5/500\n",
      "200/200 [==============================] - 0s 44us/sample - loss: 0.4711\n",
      "Epoch 6/500\n",
      "200/200 [==============================] - 0s 41us/sample - loss: 0.4540\n",
      "Epoch 7/500\n",
      "200/200 [==============================] - 0s 46us/sample - loss: 0.4371\n",
      "Epoch 8/500\n",
      "200/200 [==============================] - 0s 42us/sample - loss: 0.4219\n",
      "Epoch 9/500\n",
      "200/200 [==============================] - 0s 63us/sample - loss: 0.4069\n",
      "Epoch 10/500\n",
      "200/200 [==============================] - 0s 46us/sample - loss: 0.3929\n",
      "Epoch 11/500\n",
      "200/200 [==============================] - 0s 39us/sample - loss: 0.3804\n",
      "Epoch 12/500\n",
      "200/200 [==============================] - 0s 68us/sample - loss: 0.3681\n",
      "Epoch 13/500\n",
      "200/200 [==============================] - 0s 40us/sample - loss: 0.3563\n",
      "Epoch 14/500\n",
      "200/200 [==============================] - 0s 44us/sample - loss: 0.3451\n",
      "Epoch 15/500\n",
      "200/200 [==============================] - 0s 53us/sample - loss: 0.3349\n",
      "Epoch 16/500\n",
      "200/200 [==============================] - 0s 49us/sample - loss: 0.3252\n",
      "Epoch 17/500\n",
      "200/200 [==============================] - 0s 54us/sample - loss: 0.3157\n",
      "Epoch 18/500\n",
      "200/200 [==============================] - 0s 41us/sample - loss: 0.3071\n",
      "Epoch 19/500\n",
      "200/200 [==============================] - 0s 42us/sample - loss: 0.2978\n",
      "Epoch 20/500\n",
      "200/200 [==============================] - 0s 48us/sample - loss: 0.2892\n",
      "Epoch 21/500\n",
      "200/200 [==============================] - 0s 43us/sample - loss: 0.2811\n",
      "Epoch 22/500\n",
      "200/200 [==============================] - 0s 41us/sample - loss: 0.2731\n",
      "Epoch 23/500\n",
      "200/200 [==============================] - 0s 48us/sample - loss: 0.2654\n",
      "Epoch 24/500\n",
      "200/200 [==============================] - 0s 42us/sample - loss: 0.2578\n",
      "Epoch 25/500\n",
      "200/200 [==============================] - 0s 51us/sample - loss: 0.2504\n",
      "Epoch 26/500\n",
      "200/200 [==============================] - 0s 40us/sample - loss: 0.2433\n",
      "Epoch 27/500\n",
      "200/200 [==============================] - 0s 42us/sample - loss: 0.2368\n",
      "Epoch 28/500\n",
      "200/200 [==============================] - 0s 64us/sample - loss: 0.2303\n",
      "Epoch 29/500\n",
      "200/200 [==============================] - 0s 74us/sample - loss: 0.2241\n",
      "Epoch 30/500\n",
      "200/200 [==============================] - 0s 42us/sample - loss: 0.2185\n",
      "Epoch 31/500\n",
      "200/200 [==============================] - 0s 63us/sample - loss: 0.2127\n",
      "Epoch 32/500\n",
      "200/200 [==============================] - 0s 38us/sample - loss: 0.2072\n",
      "Epoch 33/500\n",
      "200/200 [==============================] - 0s 50us/sample - loss: 0.2023\n",
      "Epoch 34/500\n",
      "200/200 [==============================] - 0s 45us/sample - loss: 0.1970\n",
      "Epoch 35/500\n",
      "200/200 [==============================] - 0s 44us/sample - loss: 0.1920\n",
      "Epoch 36/500\n",
      "200/200 [==============================] - 0s 66us/sample - loss: 0.1872\n",
      "Epoch 37/500\n",
      "200/200 [==============================] - 0s 51us/sample - loss: 0.1826\n",
      "Epoch 38/500\n",
      "200/200 [==============================] - 0s 51us/sample - loss: 0.1784\n",
      "Epoch 39/500\n",
      "200/200 [==============================] - 0s 41us/sample - loss: 0.1741\n",
      "Epoch 40/500\n",
      "200/200 [==============================] - 0s 51us/sample - loss: 0.1701\n",
      "Epoch 41/500\n",
      "200/200 [==============================] - 0s 39us/sample - loss: 0.1661\n",
      "Epoch 42/500\n",
      "200/200 [==============================] - 0s 49us/sample - loss: 0.1623\n",
      "Epoch 43/500\n",
      "200/200 [==============================] - 0s 56us/sample - loss: 0.1586\n",
      "Epoch 44/500\n",
      "200/200 [==============================] - 0s 58us/sample - loss: 0.1551\n",
      "Epoch 45/500\n",
      "200/200 [==============================] - 0s 41us/sample - loss: 0.1517\n",
      "Epoch 46/500\n",
      "200/200 [==============================] - 0s 54us/sample - loss: 0.1483\n",
      "Epoch 47/500\n",
      "200/200 [==============================] - 0s 46us/sample - loss: 0.1451\n",
      "Epoch 48/500\n",
      "200/200 [==============================] - 0s 49us/sample - loss: 0.1421\n",
      "Epoch 49/500\n",
      "200/200 [==============================] - 0s 46us/sample - loss: 0.1392\n",
      "Epoch 50/500\n",
      "200/200 [==============================] - 0s 41us/sample - loss: 0.1361\n",
      "Epoch 51/500\n",
      "200/200 [==============================] - 0s 59us/sample - loss: 0.1333\n",
      "Epoch 52/500\n",
      "200/200 [==============================] - 0s 44us/sample - loss: 0.1306\n",
      "Epoch 53/500\n",
      "200/200 [==============================] - 0s 60us/sample - loss: 0.1279\n",
      "Epoch 54/500\n",
      "200/200 [==============================] - 0s 40us/sample - loss: 0.1253\n",
      "Epoch 55/500\n",
      "200/200 [==============================] - 0s 60us/sample - loss: 0.1229\n",
      "Epoch 56/500\n",
      "200/200 [==============================] - 0s 40us/sample - loss: 0.1205\n",
      "Epoch 57/500\n",
      "200/200 [==============================] - 0s 51us/sample - loss: 0.1181\n",
      "Epoch 58/500\n",
      "200/200 [==============================] - 0s 47us/sample - loss: 0.1159\n",
      "Epoch 59/500\n",
      "200/200 [==============================] - 0s 55us/sample - loss: 0.1137\n",
      "Epoch 60/500\n",
      "200/200 [==============================] - 0s 40us/sample - loss: 0.1117\n",
      "Epoch 61/500\n",
      "200/200 [==============================] - 0s 52us/sample - loss: 0.1097\n",
      "Epoch 62/500\n",
      "200/200 [==============================] - 0s 44us/sample - loss: 0.1077\n",
      "Epoch 63/500\n",
      "200/200 [==============================] - 0s 45us/sample - loss: 0.1059\n",
      "Epoch 64/500\n",
      "200/200 [==============================] - 0s 42us/sample - loss: 0.1041\n",
      "Epoch 65/500\n",
      "200/200 [==============================] - 0s 39us/sample - loss: 0.1023\n",
      "Epoch 66/500\n",
      "200/200 [==============================] - 0s 62us/sample - loss: 0.1007\n",
      "Epoch 67/500\n",
      "200/200 [==============================] - 0s 49us/sample - loss: 0.0990\n",
      "Epoch 68/500\n",
      "200/200 [==============================] - 0s 54us/sample - loss: 0.0974\n",
      "Epoch 69/500\n",
      "200/200 [==============================] - 0s 41us/sample - loss: 0.0959\n",
      "Epoch 70/500\n",
      "200/200 [==============================] - 0s 48us/sample - loss: 0.0944\n",
      "Epoch 71/500\n",
      "200/200 [==============================] - 0s 45us/sample - loss: 0.0930\n",
      "Epoch 72/500\n",
      "200/200 [==============================] - 0s 41us/sample - loss: 0.0916\n",
      "Epoch 73/500\n",
      "200/200 [==============================] - 0s 39us/sample - loss: 0.0903\n",
      "Epoch 74/500\n",
      "200/200 [==============================] - 0s 43us/sample - loss: 0.0889\n",
      "Epoch 75/500\n",
      "200/200 [==============================] - 0s 41us/sample - loss: 0.0877\n",
      "Epoch 76/500\n",
      "200/200 [==============================] - 0s 50us/sample - loss: 0.0864\n",
      "Epoch 77/500\n",
      "200/200 [==============================] - 0s 44us/sample - loss: 0.0852\n",
      "Epoch 78/500\n",
      "200/200 [==============================] - 0s 42us/sample - loss: 0.0841\n",
      "Epoch 79/500\n",
      "200/200 [==============================] - 0s 38us/sample - loss: 0.0830\n",
      "Epoch 80/500\n",
      "200/200 [==============================] - 0s 40us/sample - loss: 0.0820\n",
      "Epoch 81/500\n",
      "200/200 [==============================] - 0s 43us/sample - loss: 0.0810\n",
      "Epoch 82/500\n",
      "200/200 [==============================] - 0s 41us/sample - loss: 0.0799\n",
      "Epoch 83/500\n",
      "200/200 [==============================] - 0s 76us/sample - loss: 0.0789\n",
      "Epoch 84/500\n",
      "200/200 [==============================] - 0s 50us/sample - loss: 0.0779\n",
      "Epoch 85/500\n",
      "200/200 [==============================] - 0s 48us/sample - loss: 0.0770\n",
      "Epoch 86/500\n",
      "200/200 [==============================] - 0s 41us/sample - loss: 0.0762\n",
      "Epoch 87/500\n",
      "200/200 [==============================] - 0s 42us/sample - loss: 0.0754\n",
      "Epoch 88/500\n",
      "200/200 [==============================] - 0s 41us/sample - loss: 0.0745\n",
      "Epoch 89/500\n",
      "200/200 [==============================] - 0s 60us/sample - loss: 0.0738\n",
      "Epoch 90/500\n",
      "200/200 [==============================] - 0s 43us/sample - loss: 0.0730\n",
      "Epoch 91/500\n",
      "200/200 [==============================] - 0s 48us/sample - loss: 0.0723\n",
      "Epoch 92/500\n",
      "200/200 [==============================] - 0s 41us/sample - loss: 0.0716\n",
      "Epoch 93/500\n",
      "200/200 [==============================] - 0s 46us/sample - loss: 0.0709\n",
      "Epoch 94/500\n",
      "200/200 [==============================] - 0s 45us/sample - loss: 0.0702\n",
      "Epoch 95/500\n",
      "200/200 [==============================] - 0s 40us/sample - loss: 0.0695\n",
      "Epoch 96/500\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "200/200 [==============================] - 0s 43us/sample - loss: 0.0689\n",
      "Epoch 97/500\n",
      "200/200 [==============================] - 0s 43us/sample - loss: 0.0683\n",
      "Epoch 98/500\n",
      "200/200 [==============================] - 0s 41us/sample - loss: 0.0677\n",
      "Epoch 99/500\n",
      "200/200 [==============================] - 0s 42us/sample - loss: 0.0671\n",
      "Epoch 100/500\n",
      "200/200 [==============================] - 0s 39us/sample - loss: 0.0666\n",
      "Epoch 101/500\n",
      "200/200 [==============================] - 0s 40us/sample - loss: 0.0661\n",
      "Epoch 102/500\n",
      "200/200 [==============================] - 0s 42us/sample - loss: 0.0655\n",
      "Epoch 103/500\n",
      "200/200 [==============================] - 0s 43us/sample - loss: 0.0650\n",
      "Epoch 104/500\n",
      "200/200 [==============================] - 0s 42us/sample - loss: 0.0646\n",
      "Epoch 105/500\n",
      "200/200 [==============================] - 0s 37us/sample - loss: 0.0641\n",
      "Epoch 106/500\n",
      "200/200 [==============================] - 0s 38us/sample - loss: 0.0636\n",
      "Epoch 107/500\n",
      "200/200 [==============================] - 0s 40us/sample - loss: 0.0632\n",
      "Epoch 108/500\n",
      "200/200 [==============================] - 0s 42us/sample - loss: 0.0627\n",
      "Epoch 109/500\n",
      "200/200 [==============================] - 0s 42us/sample - loss: 0.0622\n",
      "Epoch 110/500\n",
      "200/200 [==============================] - 0s 43us/sample - loss: 0.0618\n",
      "Epoch 111/500\n",
      "200/200 [==============================] - 0s 41us/sample - loss: 0.0614\n",
      "Epoch 112/500\n",
      "200/200 [==============================] - 0s 40us/sample - loss: 0.0610\n",
      "Epoch 113/500\n",
      "200/200 [==============================] - 0s 41us/sample - loss: 0.0606\n",
      "Epoch 114/500\n",
      "200/200 [==============================] - 0s 47us/sample - loss: 0.0602\n",
      "Epoch 115/500\n",
      "200/200 [==============================] - 0s 60us/sample - loss: 0.0598\n",
      "Epoch 116/500\n",
      "200/200 [==============================] - 0s 49us/sample - loss: 0.0594\n",
      "Epoch 117/500\n",
      "200/200 [==============================] - 0s 49us/sample - loss: 0.0591\n",
      "Epoch 118/500\n",
      "200/200 [==============================] - 0s 39us/sample - loss: 0.0587\n",
      "Epoch 119/500\n",
      "200/200 [==============================] - 0s 53us/sample - loss: 0.0583\n",
      "Epoch 120/500\n",
      "200/200 [==============================] - 0s 45us/sample - loss: 0.0579\n",
      "Epoch 121/500\n",
      "200/200 [==============================] - 0s 39us/sample - loss: 0.0576\n",
      "Epoch 122/500\n",
      "200/200 [==============================] - 0s 43us/sample - loss: 0.0573\n",
      "Epoch 123/500\n",
      "200/200 [==============================] - 0s 39us/sample - loss: 0.0569\n",
      "Epoch 124/500\n",
      "200/200 [==============================] - 0s 41us/sample - loss: 0.0566\n",
      "Epoch 125/500\n",
      "200/200 [==============================] - 0s 42us/sample - loss: 0.0563\n",
      "Epoch 126/500\n",
      "200/200 [==============================] - 0s 43us/sample - loss: 0.0560\n",
      "Epoch 127/500\n",
      "200/200 [==============================] - 0s 42us/sample - loss: 0.0557\n",
      "Epoch 128/500\n",
      "200/200 [==============================] - 0s 43us/sample - loss: 0.0554\n",
      "Epoch 129/500\n",
      "200/200 [==============================] - 0s 40us/sample - loss: 0.0551\n",
      "Epoch 130/500\n",
      "200/200 [==============================] - 0s 50us/sample - loss: 0.0548\n",
      "Epoch 131/500\n",
      "200/200 [==============================] - 0s 39us/sample - loss: 0.0545\n",
      "Epoch 132/500\n",
      "200/200 [==============================] - 0s 45us/sample - loss: 0.0542\n",
      "Epoch 133/500\n",
      "200/200 [==============================] - 0s 49us/sample - loss: 0.0539\n",
      "Epoch 134/500\n",
      "200/200 [==============================] - 0s 48us/sample - loss: 0.0536\n",
      "Epoch 135/500\n",
      "200/200 [==============================] - 0s 63us/sample - loss: 0.0533\n",
      "Epoch 136/500\n",
      "200/200 [==============================] - 0s 43us/sample - loss: 0.0530\n",
      "Epoch 137/500\n",
      "200/200 [==============================] - 0s 49us/sample - loss: 0.0527\n",
      "Epoch 138/500\n",
      "200/200 [==============================] - 0s 51us/sample - loss: 0.0524\n",
      "Epoch 139/500\n",
      "200/200 [==============================] - 0s 71us/sample - loss: 0.0521\n",
      "Epoch 140/500\n",
      "200/200 [==============================] - 0s 43us/sample - loss: 0.0518\n",
      "Epoch 141/500\n",
      "200/200 [==============================] - 0s 38us/sample - loss: 0.0515\n",
      "Epoch 142/500\n",
      "200/200 [==============================] - 0s 47us/sample - loss: 0.0512\n",
      "Epoch 143/500\n",
      "200/200 [==============================] - 0s 58us/sample - loss: 0.0510\n",
      "Epoch 144/500\n",
      "200/200 [==============================] - 0s 52us/sample - loss: 0.0507\n",
      "Epoch 145/500\n",
      "200/200 [==============================] - 0s 80us/sample - loss: 0.0504\n",
      "Epoch 146/500\n",
      "200/200 [==============================] - 0s 44us/sample - loss: 0.0501\n",
      "Epoch 147/500\n",
      "200/200 [==============================] - 0s 62us/sample - loss: 0.0499\n",
      "Epoch 148/500\n",
      "200/200 [==============================] - 0s 38us/sample - loss: 0.0496\n",
      "Epoch 149/500\n",
      "200/200 [==============================] - 0s 58us/sample - loss: 0.0493\n",
      "Epoch 150/500\n",
      "200/200 [==============================] - 0s 44us/sample - loss: 0.0490\n",
      "Epoch 151/500\n",
      "200/200 [==============================] - 0s 45us/sample - loss: 0.0487\n",
      "Epoch 152/500\n",
      "200/200 [==============================] - 0s 55us/sample - loss: 0.0485\n",
      "Epoch 153/500\n",
      "200/200 [==============================] - 0s 49us/sample - loss: 0.0482\n",
      "Epoch 154/500\n",
      "200/200 [==============================] - 0s 49us/sample - loss: 0.0479\n",
      "Epoch 155/500\n",
      "200/200 [==============================] - 0s 37us/sample - loss: 0.0476\n",
      "Epoch 156/500\n",
      "200/200 [==============================] - 0s 51us/sample - loss: 0.0473\n",
      "Epoch 157/500\n",
      "200/200 [==============================] - 0s 47us/sample - loss: 0.0471\n",
      "Epoch 158/500\n",
      "200/200 [==============================] - 0s 64us/sample - loss: 0.0468\n",
      "Epoch 159/500\n",
      "200/200 [==============================] - 0s 45us/sample - loss: 0.0465\n",
      "Epoch 160/500\n",
      "200/200 [==============================] - 0s 53us/sample - loss: 0.0462\n",
      "Epoch 161/500\n",
      "200/200 [==============================] - 0s 44us/sample - loss: 0.0459\n",
      "Epoch 162/500\n",
      "200/200 [==============================] - 0s 62us/sample - loss: 0.0456\n",
      "Epoch 163/500\n",
      "200/200 [==============================] - 0s 49us/sample - loss: 0.0453\n",
      "Epoch 164/500\n",
      "200/200 [==============================] - 0s 48us/sample - loss: 0.0450\n",
      "Epoch 165/500\n",
      "200/200 [==============================] - 0s 36us/sample - loss: 0.0447\n",
      "Epoch 166/500\n",
      "200/200 [==============================] - 0s 45us/sample - loss: 0.0444\n",
      "Epoch 167/500\n",
      "200/200 [==============================] - 0s 47us/sample - loss: 0.0442\n",
      "Epoch 168/500\n",
      "200/200 [==============================] - 0s 44us/sample - loss: 0.0439\n",
      "Epoch 169/500\n",
      "200/200 [==============================] - 0s 65us/sample - loss: 0.0436\n",
      "Epoch 170/500\n",
      "200/200 [==============================] - 0s 48us/sample - loss: 0.0433\n",
      "Epoch 171/500\n",
      "200/200 [==============================] - 0s 57us/sample - loss: 0.0430\n",
      "Epoch 172/500\n",
      "200/200 [==============================] - 0s 43us/sample - loss: 0.0427\n",
      "Epoch 173/500\n",
      "200/200 [==============================] - 0s 73us/sample - loss: 0.0424\n",
      "Epoch 174/500\n",
      "200/200 [==============================] - 0s 39us/sample - loss: 0.0421\n",
      "Epoch 175/500\n",
      "200/200 [==============================] - 0s 46us/sample - loss: 0.0418\n",
      "Epoch 176/500\n",
      "200/200 [==============================] - 0s 58us/sample - loss: 0.0415\n",
      "Epoch 177/500\n",
      "200/200 [==============================] - 0s 39us/sample - loss: 0.0412\n",
      "Epoch 178/500\n",
      "200/200 [==============================] - 0s 37us/sample - loss: 0.0409\n",
      "Epoch 179/500\n",
      "200/200 [==============================] - 0s 50us/sample - loss: 0.0406\n",
      "Epoch 180/500\n",
      "200/200 [==============================] - 0s 58us/sample - loss: 0.0403\n",
      "Epoch 181/500\n",
      "200/200 [==============================] - 0s 44us/sample - loss: 0.0399\n",
      "Epoch 182/500\n",
      "200/200 [==============================] - 0s 52us/sample - loss: 0.0396\n",
      "Epoch 183/500\n",
      "200/200 [==============================] - 0s 40us/sample - loss: 0.0393\n",
      "Epoch 184/500\n",
      "200/200 [==============================] - 0s 52us/sample - loss: 0.0390\n",
      "Epoch 185/500\n",
      "200/200 [==============================] - 0s 41us/sample - loss: 0.0387\n",
      "Epoch 186/500\n",
      "200/200 [==============================] - 0s 41us/sample - loss: 0.0384\n",
      "Epoch 187/500\n",
      "200/200 [==============================] - 0s 45us/sample - loss: 0.0381\n",
      "Epoch 188/500\n",
      "200/200 [==============================] - 0s 47us/sample - loss: 0.0378\n",
      "Epoch 189/500\n",
      "200/200 [==============================] - 0s 43us/sample - loss: 0.0375\n",
      "Epoch 190/500\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "200/200 [==============================] - 0s 41us/sample - loss: 0.0371\n",
      "Epoch 191/500\n",
      "200/200 [==============================] - 0s 41us/sample - loss: 0.0368\n",
      "Epoch 192/500\n",
      "200/200 [==============================] - 0s 44us/sample - loss: 0.0365\n",
      "Epoch 193/500\n",
      "200/200 [==============================] - 0s 47us/sample - loss: 0.0362\n",
      "Epoch 194/500\n",
      "200/200 [==============================] - 0s 40us/sample - loss: 0.0359\n",
      "Epoch 195/500\n",
      "200/200 [==============================] - 0s 52us/sample - loss: 0.0356\n",
      "Epoch 196/500\n",
      "200/200 [==============================] - 0s 41us/sample - loss: 0.0353\n",
      "Epoch 197/500\n",
      "200/200 [==============================] - 0s 39us/sample - loss: 0.0349\n",
      "Epoch 198/500\n",
      "200/200 [==============================] - 0s 41us/sample - loss: 0.0346\n",
      "Epoch 199/500\n",
      "200/200 [==============================] - 0s 43us/sample - loss: 0.0343\n",
      "Epoch 200/500\n",
      "200/200 [==============================] - 0s 46us/sample - loss: 0.0340\n",
      "Epoch 201/500\n",
      "200/200 [==============================] - 0s 44us/sample - loss: 0.0337\n",
      "Epoch 202/500\n",
      "200/200 [==============================] - 0s 72us/sample - loss: 0.0334\n",
      "Epoch 203/500\n",
      "200/200 [==============================] - 0s 44us/sample - loss: 0.0331\n",
      "Epoch 204/500\n",
      "200/200 [==============================] - 0s 41us/sample - loss: 0.0327\n",
      "Epoch 205/500\n",
      "200/200 [==============================] - 0s 53us/sample - loss: 0.0324\n",
      "Epoch 206/500\n",
      "200/200 [==============================] - 0s 41us/sample - loss: 0.0321\n",
      "Epoch 207/500\n",
      "200/200 [==============================] - 0s 46us/sample - loss: 0.0318\n",
      "Epoch 208/500\n",
      "200/200 [==============================] - 0s 46us/sample - loss: 0.0315\n",
      "Epoch 209/500\n",
      "200/200 [==============================] - 0s 38us/sample - loss: 0.0312\n",
      "Epoch 210/500\n",
      "200/200 [==============================] - 0s 42us/sample - loss: 0.0309\n",
      "Epoch 211/500\n",
      "200/200 [==============================] - 0s 49us/sample - loss: 0.0305\n",
      "Epoch 212/500\n",
      "200/200 [==============================] - 0s 52us/sample - loss: 0.0302\n",
      "Epoch 213/500\n",
      "200/200 [==============================] - 0s 46us/sample - loss: 0.0299\n",
      "Epoch 214/500\n",
      "200/200 [==============================] - 0s 50us/sample - loss: 0.0296\n",
      "Epoch 215/500\n",
      "200/200 [==============================] - 0s 52us/sample - loss: 0.0293\n",
      "Epoch 216/500\n",
      "200/200 [==============================] - 0s 43us/sample - loss: 0.0290\n",
      "Epoch 217/500\n",
      "200/200 [==============================] - 0s 45us/sample - loss: 0.0287\n",
      "Epoch 218/500\n",
      "200/200 [==============================] - 0s 51us/sample - loss: 0.0284\n",
      "Epoch 219/500\n",
      "200/200 [==============================] - 0s 47us/sample - loss: 0.0281\n",
      "Epoch 220/500\n",
      "200/200 [==============================] - 0s 46us/sample - loss: 0.0278\n",
      "Epoch 221/500\n",
      "200/200 [==============================] - 0s 43us/sample - loss: 0.0275\n",
      "Epoch 222/500\n",
      "200/200 [==============================] - 0s 50us/sample - loss: 0.0272\n",
      "Epoch 223/500\n",
      "200/200 [==============================] - 0s 49us/sample - loss: 0.0269\n",
      "Epoch 224/500\n",
      "200/200 [==============================] - 0s 51us/sample - loss: 0.0266\n",
      "Epoch 225/500\n",
      "200/200 [==============================] - 0s 56us/sample - loss: 0.0263\n",
      "Epoch 226/500\n",
      "200/200 [==============================] - 0s 46us/sample - loss: 0.0260\n",
      "Epoch 227/500\n",
      "200/200 [==============================] - 0s 48us/sample - loss: 0.0257\n",
      "Epoch 228/500\n",
      "200/200 [==============================] - 0s 39us/sample - loss: 0.0254\n",
      "Epoch 229/500\n",
      "200/200 [==============================] - 0s 40us/sample - loss: 0.0251\n",
      "Epoch 230/500\n",
      "200/200 [==============================] - 0s 46us/sample - loss: 0.0249\n",
      "Epoch 231/500\n",
      "200/200 [==============================] - 0s 44us/sample - loss: 0.0246\n",
      "Epoch 232/500\n",
      "200/200 [==============================] - 0s 43us/sample - loss: 0.0243\n",
      "Epoch 233/500\n",
      "200/200 [==============================] - 0s 46us/sample - loss: 0.0240\n",
      "Epoch 234/500\n",
      "200/200 [==============================] - 0s 46us/sample - loss: 0.0237\n",
      "Epoch 235/500\n",
      "200/200 [==============================] - 0s 77us/sample - loss: 0.0234\n",
      "Epoch 236/500\n",
      "200/200 [==============================] - 0s 46us/sample - loss: 0.0231\n",
      "Epoch 237/500\n",
      "200/200 [==============================] - 0s 54us/sample - loss: 0.0228\n",
      "Epoch 238/500\n",
      "200/200 [==============================] - 0s 44us/sample - loss: 0.0226\n",
      "Epoch 239/500\n",
      "200/200 [==============================] - 0s 45us/sample - loss: 0.0223\n",
      "Epoch 240/500\n",
      "200/200 [==============================] - 0s 44us/sample - loss: 0.0220\n",
      "Epoch 241/500\n",
      "200/200 [==============================] - 0s 45us/sample - loss: 0.0218\n",
      "Epoch 242/500\n",
      "200/200 [==============================] - 0s 48us/sample - loss: 0.0215\n",
      "Epoch 243/500\n",
      "200/200 [==============================] - 0s 52us/sample - loss: 0.0212\n",
      "Epoch 244/500\n",
      "200/200 [==============================] - 0s 44us/sample - loss: 0.0210\n",
      "Epoch 245/500\n",
      "200/200 [==============================] - 0s 41us/sample - loss: 0.0207\n",
      "Epoch 246/500\n",
      "200/200 [==============================] - 0s 62us/sample - loss: 0.0204\n",
      "Epoch 247/500\n",
      "200/200 [==============================] - 0s 69us/sample - loss: 0.0202\n",
      "Epoch 248/500\n",
      "200/200 [==============================] - 0s 51us/sample - loss: 0.0199\n",
      "Epoch 249/500\n",
      "200/200 [==============================] - 0s 43us/sample - loss: 0.0197\n",
      "Epoch 250/500\n",
      "200/200 [==============================] - 0s 52us/sample - loss: 0.0194\n",
      "Epoch 251/500\n",
      "200/200 [==============================] - 0s 41us/sample - loss: 0.0192\n",
      "Epoch 252/500\n",
      "200/200 [==============================] - 0s 50us/sample - loss: 0.0189\n",
      "Epoch 253/500\n",
      "200/200 [==============================] - 0s 44us/sample - loss: 0.0187\n",
      "Epoch 254/500\n",
      "200/200 [==============================] - 0s 50us/sample - loss: 0.0185\n",
      "Epoch 255/500\n",
      "200/200 [==============================] - 0s 45us/sample - loss: 0.0182\n",
      "Epoch 256/500\n",
      "200/200 [==============================] - 0s 52us/sample - loss: 0.0180\n",
      "Epoch 257/500\n",
      "200/200 [==============================] - 0s 43us/sample - loss: 0.0178\n",
      "Epoch 258/500\n",
      "200/200 [==============================] - 0s 42us/sample - loss: 0.0175\n",
      "Epoch 259/500\n",
      "200/200 [==============================] - 0s 52us/sample - loss: 0.0173\n",
      "Epoch 260/500\n",
      "200/200 [==============================] - 0s 38us/sample - loss: 0.0171\n",
      "Epoch 261/500\n",
      "200/200 [==============================] - 0s 64us/sample - loss: 0.0169\n",
      "Epoch 262/500\n",
      "200/200 [==============================] - 0s 40us/sample - loss: 0.0167\n",
      "Epoch 263/500\n",
      "200/200 [==============================] - 0s 50us/sample - loss: 0.0165\n",
      "Epoch 264/500\n",
      "200/200 [==============================] - 0s 41us/sample - loss: 0.0163\n",
      "Epoch 265/500\n",
      "200/200 [==============================] - 0s 63us/sample - loss: 0.0161\n",
      "Epoch 266/500\n",
      "200/200 [==============================] - 0s 41us/sample - loss: 0.0159\n",
      "Epoch 267/500\n",
      "200/200 [==============================] - 0s 54us/sample - loss: 0.0157\n",
      "Epoch 268/500\n",
      "200/200 [==============================] - 0s 40us/sample - loss: 0.0155\n",
      "Epoch 269/500\n",
      "200/200 [==============================] - 0s 48us/sample - loss: 0.0153\n",
      "Epoch 270/500\n",
      "200/200 [==============================] - 0s 43us/sample - loss: 0.0151\n",
      "Epoch 271/500\n",
      "200/200 [==============================] - 0s 55us/sample - loss: 0.0149\n",
      "Epoch 272/500\n",
      "200/200 [==============================] - 0s 44us/sample - loss: 0.0147\n",
      "Epoch 273/500\n",
      "200/200 [==============================] - 0s 51us/sample - loss: 0.0145\n",
      "Epoch 274/500\n",
      "200/200 [==============================] - 0s 46us/sample - loss: 0.0144\n",
      "Epoch 275/500\n",
      "200/200 [==============================] - 0s 48us/sample - loss: 0.0142\n",
      "Epoch 276/500\n",
      "200/200 [==============================] - 0s 46us/sample - loss: 0.0140\n",
      "Epoch 277/500\n",
      "200/200 [==============================] - 0s 49us/sample - loss: 0.0138\n",
      "Epoch 278/500\n",
      "200/200 [==============================] - 0s 47us/sample - loss: 0.0137\n",
      "Epoch 279/500\n",
      "200/200 [==============================] - 0s 47us/sample - loss: 0.0135\n",
      "Epoch 280/500\n",
      "200/200 [==============================] - 0s 56us/sample - loss: 0.0133\n",
      "Epoch 281/500\n",
      "200/200 [==============================] - 0s 52us/sample - loss: 0.0132\n",
      "Epoch 282/500\n",
      "200/200 [==============================] - 0s 41us/sample - loss: 0.0130\n",
      "Epoch 283/500\n",
      "200/200 [==============================] - 0s 56us/sample - loss: 0.0129\n",
      "Epoch 284/500\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "200/200 [==============================] - 0s 42us/sample - loss: 0.0127\n",
      "Epoch 285/500\n",
      "200/200 [==============================] - 0s 58us/sample - loss: 0.0125\n",
      "Epoch 286/500\n",
      "200/200 [==============================] - 0s 50us/sample - loss: 0.0124\n",
      "Epoch 287/500\n",
      "200/200 [==============================] - 0s 45us/sample - loss: 0.0123\n",
      "Epoch 288/500\n",
      "200/200 [==============================] - 0s 52us/sample - loss: 0.0121\n",
      "Epoch 289/500\n",
      "200/200 [==============================] - 0s 44us/sample - loss: 0.0120\n",
      "Epoch 290/500\n",
      "200/200 [==============================] - 0s 41us/sample - loss: 0.0119\n",
      "Epoch 291/500\n",
      "200/200 [==============================] - 0s 43us/sample - loss: 0.0117\n",
      "Epoch 292/500\n",
      "200/200 [==============================] - 0s 42us/sample - loss: 0.0116\n",
      "Epoch 293/500\n",
      "200/200 [==============================] - 0s 49us/sample - loss: 0.0115\n",
      "Epoch 294/500\n",
      "200/200 [==============================] - 0s 41us/sample - loss: 0.0113\n",
      "Epoch 295/500\n",
      "200/200 [==============================] - 0s 42us/sample - loss: 0.0112\n",
      "Epoch 296/500\n",
      "200/200 [==============================] - 0s 50us/sample - loss: 0.0111\n",
      "Epoch 297/500\n",
      "200/200 [==============================] - 0s 39us/sample - loss: 0.0110\n",
      "Epoch 298/500\n",
      "200/200 [==============================] - 0s 37us/sample - loss: 0.0109\n",
      "Epoch 299/500\n",
      "200/200 [==============================] - 0s 50us/sample - loss: 0.0108\n",
      "Epoch 300/500\n",
      "200/200 [==============================] - 0s 43us/sample - loss: 0.0107\n",
      "Epoch 301/500\n",
      "200/200 [==============================] - 0s 40us/sample - loss: 0.0105\n",
      "Epoch 302/500\n",
      "200/200 [==============================] - 0s 45us/sample - loss: 0.0104\n",
      "Epoch 303/500\n",
      "200/200 [==============================] - 0s 46us/sample - loss: 0.0103\n",
      "Epoch 304/500\n",
      "200/200 [==============================] - 0s 43us/sample - loss: 0.0102\n",
      "Epoch 305/500\n",
      "200/200 [==============================] - 0s 45us/sample - loss: 0.0101\n",
      "Epoch 306/500\n",
      "200/200 [==============================] - 0s 44us/sample - loss: 0.0101\n",
      "Epoch 307/500\n",
      "200/200 [==============================] - 0s 40us/sample - loss: 0.0100\n",
      "Epoch 308/500\n",
      "200/200 [==============================] - 0s 40us/sample - loss: 0.0099\n",
      "Epoch 309/500\n",
      "200/200 [==============================] - 0s 42us/sample - loss: 0.0098\n",
      "Epoch 310/500\n",
      "200/200 [==============================] - 0s 40us/sample - loss: 0.0097\n",
      "Epoch 311/500\n",
      "200/200 [==============================] - 0s 42us/sample - loss: 0.0096\n",
      "Epoch 312/500\n",
      "200/200 [==============================] - 0s 38us/sample - loss: 0.0095\n",
      "Epoch 313/500\n",
      "200/200 [==============================] - 0s 41us/sample - loss: 0.0094\n",
      "Epoch 314/500\n",
      "200/200 [==============================] - 0s 41us/sample - loss: 0.0094\n",
      "Epoch 315/500\n",
      "200/200 [==============================] - 0s 61us/sample - loss: 0.0093\n",
      "Epoch 316/500\n",
      "200/200 [==============================] - 0s 41us/sample - loss: 0.0092\n",
      "Epoch 317/500\n",
      "200/200 [==============================] - 0s 46us/sample - loss: 0.0091\n",
      "Epoch 318/500\n",
      "200/200 [==============================] - 0s 46us/sample - loss: 0.0091\n",
      "Epoch 319/500\n",
      "200/200 [==============================] - 0s 49us/sample - loss: 0.0090\n",
      "Epoch 320/500\n",
      "200/200 [==============================] - 0s 42us/sample - loss: 0.0089\n",
      "Epoch 321/500\n",
      "200/200 [==============================] - 0s 43us/sample - loss: 0.0089\n",
      "Epoch 322/500\n",
      "200/200 [==============================] - 0s 40us/sample - loss: 0.0088\n",
      "Epoch 323/500\n",
      "200/200 [==============================] - 0s 40us/sample - loss: 0.0087\n",
      "Epoch 324/500\n",
      "200/200 [==============================] - 0s 45us/sample - loss: 0.0087\n",
      "Epoch 325/500\n",
      "200/200 [==============================] - 0s 47us/sample - loss: 0.0086\n",
      "Epoch 326/500\n",
      "200/200 [==============================] - 0s 38us/sample - loss: 0.0085\n",
      "Epoch 327/500\n",
      "200/200 [==============================] - 0s 41us/sample - loss: 0.0085\n",
      "Epoch 328/500\n",
      "200/200 [==============================] - 0s 40us/sample - loss: 0.0084\n",
      "Epoch 329/500\n",
      "200/200 [==============================] - 0s 44us/sample - loss: 0.0084\n",
      "Epoch 330/500\n",
      "200/200 [==============================] - 0s 46us/sample - loss: 0.0083\n",
      "Epoch 331/500\n",
      "200/200 [==============================] - 0s 48us/sample - loss: 0.0082\n",
      "Epoch 332/500\n",
      "200/200 [==============================] - 0s 45us/sample - loss: 0.0082\n",
      "Epoch 333/500\n",
      "200/200 [==============================] - 0s 68us/sample - loss: 0.0081\n",
      "Epoch 334/500\n",
      "200/200 [==============================] - 0s 44us/sample - loss: 0.0081\n",
      "Epoch 335/500\n",
      "200/200 [==============================] - 0s 50us/sample - loss: 0.0080\n",
      "Epoch 336/500\n",
      "200/200 [==============================] - 0s 42us/sample - loss: 0.0080\n",
      "Epoch 337/500\n",
      "200/200 [==============================] - 0s 40us/sample - loss: 0.0079\n",
      "Epoch 338/500\n",
      "200/200 [==============================] - 0s 43us/sample - loss: 0.0079\n",
      "Epoch 339/500\n",
      "200/200 [==============================] - 0s 47us/sample - loss: 0.0078\n",
      "Epoch 340/500\n",
      "200/200 [==============================] - 0s 61us/sample - loss: 0.0078\n",
      "Epoch 341/500\n",
      "200/200 [==============================] - 0s 52us/sample - loss: 0.0077\n",
      "Epoch 342/500\n",
      "200/200 [==============================] - 0s 38us/sample - loss: 0.0077\n",
      "Epoch 343/500\n",
      "200/200 [==============================] - 0s 53us/sample - loss: 0.0076\n",
      "Epoch 344/500\n",
      "200/200 [==============================] - 0s 52us/sample - loss: 0.0076\n",
      "Epoch 345/500\n",
      "200/200 [==============================] - 0s 38us/sample - loss: 0.0076\n",
      "Epoch 346/500\n",
      "200/200 [==============================] - 0s 40us/sample - loss: 0.0075\n",
      "Epoch 347/500\n",
      "200/200 [==============================] - 0s 56us/sample - loss: 0.0075\n",
      "Epoch 348/500\n",
      "200/200 [==============================] - 0s 42us/sample - loss: 0.0074\n",
      "Epoch 349/500\n",
      "200/200 [==============================] - 0s 61us/sample - loss: 0.0074\n",
      "Epoch 350/500\n",
      "200/200 [==============================] - 0s 43us/sample - loss: 0.0073\n",
      "Epoch 351/500\n",
      "200/200 [==============================] - 0s 64us/sample - loss: 0.0073\n",
      "Epoch 352/500\n",
      "200/200 [==============================] - 0s 39us/sample - loss: 0.0073\n",
      "Epoch 353/500\n",
      "200/200 [==============================] - 0s 50us/sample - loss: 0.0072\n",
      "Epoch 354/500\n",
      "200/200 [==============================] - 0s 43us/sample - loss: 0.0072\n",
      "Epoch 355/500\n",
      "200/200 [==============================] - 0s 48us/sample - loss: 0.0072\n",
      "Epoch 356/500\n",
      "200/200 [==============================] - 0s 38us/sample - loss: 0.0071\n",
      "Epoch 357/500\n",
      "200/200 [==============================] - 0s 70us/sample - loss: 0.0071\n",
      "Epoch 358/500\n",
      "200/200 [==============================] - 0s 40us/sample - loss: 0.0070\n",
      "Epoch 359/500\n",
      "200/200 [==============================] - 0s 47us/sample - loss: 0.0070\n",
      "Epoch 360/500\n",
      "200/200 [==============================] - 0s 56us/sample - loss: 0.0070\n",
      "Epoch 361/500\n",
      "200/200 [==============================] - 0s 50us/sample - loss: 0.0069\n",
      "Epoch 362/500\n",
      "200/200 [==============================] - 0s 52us/sample - loss: 0.0069\n",
      "Epoch 363/500\n",
      "200/200 [==============================] - 0s 56us/sample - loss: 0.0069\n",
      "Epoch 364/500\n",
      "200/200 [==============================] - 0s 49us/sample - loss: 0.0068\n",
      "Epoch 365/500\n",
      "200/200 [==============================] - 0s 59us/sample - loss: 0.0068\n",
      "Epoch 366/500\n",
      "200/200 [==============================] - 0s 46us/sample - loss: 0.0068\n",
      "Epoch 367/500\n",
      "200/200 [==============================] - 0s 51us/sample - loss: 0.0067\n",
      "Epoch 368/500\n",
      "200/200 [==============================] - 0s 53us/sample - loss: 0.0067\n",
      "Epoch 369/500\n",
      "200/200 [==============================] - 0s 48us/sample - loss: 0.0067\n",
      "Epoch 370/500\n",
      "200/200 [==============================] - 0s 40us/sample - loss: 0.0066\n",
      "Epoch 371/500\n",
      "200/200 [==============================] - 0s 38us/sample - loss: 0.0066\n",
      "Epoch 372/500\n",
      "200/200 [==============================] - 0s 62us/sample - loss: 0.0066\n",
      "Epoch 373/500\n",
      "200/200 [==============================] - 0s 60us/sample - loss: 0.0066\n",
      "Epoch 374/500\n",
      "200/200 [==============================] - 0s 43us/sample - loss: 0.0065\n",
      "Epoch 375/500\n",
      "200/200 [==============================] - 0s 46us/sample - loss: 0.0065\n",
      "Epoch 376/500\n",
      "200/200 [==============================] - 0s 46us/sample - loss: 0.0065\n",
      "Epoch 377/500\n",
      "200/200 [==============================] - 0s 47us/sample - loss: 0.0064\n",
      "Epoch 378/500\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "200/200 [==============================] - 0s 47us/sample - loss: 0.0064\n",
      "Epoch 379/500\n",
      "200/200 [==============================] - 0s 49us/sample - loss: 0.0064\n",
      "Epoch 380/500\n",
      "200/200 [==============================] - 0s 51us/sample - loss: 0.0064\n",
      "Epoch 381/500\n",
      "200/200 [==============================] - 0s 54us/sample - loss: 0.0063\n",
      "Epoch 382/500\n",
      "200/200 [==============================] - 0s 57us/sample - loss: 0.0063\n",
      "Epoch 383/500\n",
      "200/200 [==============================] - 0s 44us/sample - loss: 0.0063\n",
      "Epoch 384/500\n",
      "200/200 [==============================] - 0s 41us/sample - loss: 0.0062\n",
      "Epoch 385/500\n",
      "200/200 [==============================] - 0s 51us/sample - loss: 0.0062\n",
      "Epoch 386/500\n",
      "200/200 [==============================] - 0s 40us/sample - loss: 0.0062\n",
      "Epoch 387/500\n",
      "200/200 [==============================] - 0s 38us/sample - loss: 0.0062\n",
      "Epoch 388/500\n",
      "200/200 [==============================] - 0s 40us/sample - loss: 0.0061\n",
      "Epoch 389/500\n",
      "200/200 [==============================] - 0s 45us/sample - loss: 0.0061\n",
      "Epoch 390/500\n",
      "200/200 [==============================] - 0s 54us/sample - loss: 0.0061\n",
      "Epoch 391/500\n",
      "200/200 [==============================] - 0s 51us/sample - loss: 0.0061\n",
      "Epoch 392/500\n",
      "200/200 [==============================] - 0s 40us/sample - loss: 0.0060\n",
      "Epoch 393/500\n",
      "200/200 [==============================] - 0s 40us/sample - loss: 0.0060\n",
      "Epoch 394/500\n",
      "200/200 [==============================] - 0s 58us/sample - loss: 0.0060\n",
      "Epoch 395/500\n",
      "200/200 [==============================] - 0s 53us/sample - loss: 0.0060\n",
      "Epoch 396/500\n",
      "200/200 [==============================] - 0s 43us/sample - loss: 0.0059\n",
      "Epoch 397/500\n",
      "200/200 [==============================] - 0s 45us/sample - loss: 0.0059\n",
      "Epoch 398/500\n",
      "200/200 [==============================] - 0s 62us/sample - loss: 0.0059\n",
      "Epoch 399/500\n",
      "200/200 [==============================] - 0s 42us/sample - loss: 0.0059\n",
      "Epoch 400/500\n",
      "200/200 [==============================] - 0s 44us/sample - loss: 0.0058\n",
      "Epoch 401/500\n",
      "200/200 [==============================] - 0s 48us/sample - loss: 0.0058\n",
      "Epoch 402/500\n",
      "200/200 [==============================] - 0s 44us/sample - loss: 0.0058\n",
      "Epoch 403/500\n",
      "200/200 [==============================] - 0s 45us/sample - loss: 0.0058\n",
      "Epoch 404/500\n",
      "200/200 [==============================] - 0s 52us/sample - loss: 0.0057\n",
      "Epoch 405/500\n",
      "200/200 [==============================] - 0s 51us/sample - loss: 0.0057\n",
      "Epoch 406/500\n",
      "200/200 [==============================] - 0s 42us/sample - loss: 0.0057\n",
      "Epoch 407/500\n",
      "200/200 [==============================] - 0s 47us/sample - loss: 0.0057\n",
      "Epoch 408/500\n",
      "200/200 [==============================] - 0s 44us/sample - loss: 0.0056\n",
      "Epoch 409/500\n",
      "200/200 [==============================] - 0s 44us/sample - loss: 0.0056\n",
      "Epoch 410/500\n",
      "200/200 [==============================] - 0s 37us/sample - loss: 0.0056\n",
      "Epoch 411/500\n",
      "200/200 [==============================] - 0s 38us/sample - loss: 0.0056\n",
      "Epoch 412/500\n",
      "200/200 [==============================] - 0s 38us/sample - loss: 0.0056\n",
      "Epoch 413/500\n",
      "200/200 [==============================] - 0s 41us/sample - loss: 0.0055\n",
      "Epoch 414/500\n",
      "200/200 [==============================] - 0s 49us/sample - loss: 0.0055\n",
      "Epoch 415/500\n",
      "200/200 [==============================] - 0s 46us/sample - loss: 0.0055\n",
      "Epoch 416/500\n",
      "200/200 [==============================] - 0s 41us/sample - loss: 0.0055\n",
      "Epoch 417/500\n",
      "200/200 [==============================] - 0s 39us/sample - loss: 0.0055\n",
      "Epoch 418/500\n",
      "200/200 [==============================] - 0s 48us/sample - loss: 0.0054\n",
      "Epoch 419/500\n",
      "200/200 [==============================] - 0s 43us/sample - loss: 0.0054\n",
      "Epoch 420/500\n",
      "200/200 [==============================] - 0s 47us/sample - loss: 0.0054\n",
      "Epoch 421/500\n",
      "200/200 [==============================] - 0s 50us/sample - loss: 0.0054\n",
      "Epoch 422/500\n",
      "200/200 [==============================] - 0s 47us/sample - loss: 0.0054\n",
      "Epoch 423/500\n",
      "200/200 [==============================] - 0s 50us/sample - loss: 0.0053\n",
      "Epoch 424/500\n",
      "200/200 [==============================] - 0s 47us/sample - loss: 0.0053\n",
      "Epoch 425/500\n",
      "200/200 [==============================] - 0s 50us/sample - loss: 0.0053\n",
      "Epoch 426/500\n",
      "200/200 [==============================] - 0s 46us/sample - loss: 0.0053\n",
      "Epoch 427/500\n",
      "200/200 [==============================] - 0s 43us/sample - loss: 0.0052\n",
      "Epoch 428/500\n",
      "200/200 [==============================] - 0s 44us/sample - loss: 0.0052\n",
      "Epoch 429/500\n",
      "200/200 [==============================] - 0s 42us/sample - loss: 0.0052\n",
      "Epoch 430/500\n",
      "200/200 [==============================] - 0s 42us/sample - loss: 0.0052\n",
      "Epoch 431/500\n",
      "200/200 [==============================] - 0s 49us/sample - loss: 0.0052\n",
      "Epoch 432/500\n",
      "200/200 [==============================] - 0s 50us/sample - loss: 0.0051\n",
      "Epoch 433/500\n",
      "200/200 [==============================] - 0s 49us/sample - loss: 0.0051\n",
      "Epoch 434/500\n",
      "200/200 [==============================] - 0s 43us/sample - loss: 0.0051\n",
      "Epoch 435/500\n",
      "200/200 [==============================] - 0s 41us/sample - loss: 0.0051\n",
      "Epoch 436/500\n",
      "200/200 [==============================] - 0s 47us/sample - loss: 0.0051\n",
      "Epoch 437/500\n",
      "200/200 [==============================] - 0s 51us/sample - loss: 0.0050\n",
      "Epoch 438/500\n",
      "200/200 [==============================] - 0s 46us/sample - loss: 0.0050\n",
      "Epoch 439/500\n",
      "200/200 [==============================] - 0s 45us/sample - loss: 0.0050\n",
      "Epoch 440/500\n",
      "200/200 [==============================] - 0s 45us/sample - loss: 0.0050\n",
      "Epoch 441/500\n",
      "200/200 [==============================] - 0s 40us/sample - loss: 0.0050\n",
      "Epoch 442/500\n",
      "200/200 [==============================] - 0s 42us/sample - loss: 0.0049\n",
      "Epoch 443/500\n",
      "200/200 [==============================] - 0s 45us/sample - loss: 0.0049\n",
      "Epoch 444/500\n",
      "200/200 [==============================] - 0s 42us/sample - loss: 0.0049\n",
      "Epoch 445/500\n",
      "200/200 [==============================] - 0s 41us/sample - loss: 0.0049\n",
      "Epoch 446/500\n",
      "200/200 [==============================] - 0s 46us/sample - loss: 0.0049\n",
      "Epoch 447/500\n",
      "200/200 [==============================] - 0s 49us/sample - loss: 0.0048\n",
      "Epoch 448/500\n",
      "200/200 [==============================] - 0s 43us/sample - loss: 0.0048\n",
      "Epoch 449/500\n",
      "200/200 [==============================] - 0s 40us/sample - loss: 0.0048\n",
      "Epoch 450/500\n",
      "200/200 [==============================] - 0s 61us/sample - loss: 0.0048\n",
      "Epoch 451/500\n",
      "200/200 [==============================] - 0s 42us/sample - loss: 0.0048\n",
      "Epoch 452/500\n",
      "200/200 [==============================] - 0s 57us/sample - loss: 0.0047\n",
      "Epoch 453/500\n",
      "200/200 [==============================] - 0s 50us/sample - loss: 0.0047\n",
      "Epoch 454/500\n",
      "200/200 [==============================] - 0s 56us/sample - loss: 0.0047\n",
      "Epoch 455/500\n",
      "200/200 [==============================] - 0s 42us/sample - loss: 0.0047\n",
      "Epoch 456/500\n",
      "200/200 [==============================] - 0s 53us/sample - loss: 0.0047\n",
      "Epoch 457/500\n",
      "200/200 [==============================] - 0s 55us/sample - loss: 0.0047\n",
      "Epoch 458/500\n",
      "200/200 [==============================] - 0s 43us/sample - loss: 0.0046\n",
      "Epoch 459/500\n",
      "200/200 [==============================] - 0s 46us/sample - loss: 0.0046\n",
      "Epoch 460/500\n",
      "200/200 [==============================] - 0s 76us/sample - loss: 0.0046\n",
      "Epoch 461/500\n",
      "200/200 [==============================] - 0s 48us/sample - loss: 0.0046\n",
      "Epoch 462/500\n",
      "200/200 [==============================] - 0s 45us/sample - loss: 0.0046\n",
      "Epoch 463/500\n",
      "200/200 [==============================] - 0s 42us/sample - loss: 0.0045\n",
      "Epoch 464/500\n",
      "200/200 [==============================] - 0s 57us/sample - loss: 0.0045\n",
      "Epoch 465/500\n",
      "200/200 [==============================] - 0s 39us/sample - loss: 0.0045\n",
      "Epoch 466/500\n",
      "200/200 [==============================] - 0s 69us/sample - loss: 0.0045\n",
      "Epoch 467/500\n",
      "200/200 [==============================] - 0s 45us/sample - loss: 0.0045\n",
      "Epoch 468/500\n",
      "200/200 [==============================] - 0s 48us/sample - loss: 0.0045\n",
      "Epoch 469/500\n",
      "200/200 [==============================] - 0s 39us/sample - loss: 0.0044\n",
      "Epoch 470/500\n",
      "200/200 [==============================] - 0s 49us/sample - loss: 0.0044\n",
      "Epoch 471/500\n",
      "200/200 [==============================] - 0s 44us/sample - loss: 0.0044\n",
      "Epoch 472/500\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "200/200 [==============================] - 0s 43us/sample - loss: 0.0044\n",
      "Epoch 473/500\n",
      "200/200 [==============================] - 0s 65us/sample - loss: 0.0044\n",
      "Epoch 474/500\n",
      "200/200 [==============================] - 0s 39us/sample - loss: 0.0044\n",
      "Epoch 475/500\n",
      "200/200 [==============================] - 0s 46us/sample - loss: 0.0043\n",
      "Epoch 476/500\n",
      "200/200 [==============================] - 0s 44us/sample - loss: 0.0043\n",
      "Epoch 477/500\n",
      "200/200 [==============================] - 0s 41us/sample - loss: 0.0043\n",
      "Epoch 478/500\n",
      "200/200 [==============================] - 0s 50us/sample - loss: 0.0043\n",
      "Epoch 479/500\n",
      "200/200 [==============================] - 0s 43us/sample - loss: 0.0043\n",
      "Epoch 480/500\n",
      "200/200 [==============================] - 0s 39us/sample - loss: 0.0042\n",
      "Epoch 481/500\n",
      "200/200 [==============================] - 0s 53us/sample - loss: 0.0042\n",
      "Epoch 482/500\n",
      "200/200 [==============================] - 0s 40us/sample - loss: 0.0042\n",
      "Epoch 483/500\n",
      "200/200 [==============================] - 0s 42us/sample - loss: 0.0042\n",
      "Epoch 484/500\n",
      "200/200 [==============================] - 0s 56us/sample - loss: 0.0042\n",
      "Epoch 485/500\n",
      "200/200 [==============================] - 0s 39us/sample - loss: 0.0042\n",
      "Epoch 486/500\n",
      "200/200 [==============================] - 0s 41us/sample - loss: 0.0041\n",
      "Epoch 487/500\n",
      "200/200 [==============================] - 0s 60us/sample - loss: 0.0041\n",
      "Epoch 488/500\n",
      "200/200 [==============================] - 0s 42us/sample - loss: 0.0041\n",
      "Epoch 489/500\n",
      "200/200 [==============================] - 0s 71us/sample - loss: 0.0041\n",
      "Epoch 490/500\n",
      "200/200 [==============================] - 0s 54us/sample - loss: 0.0041\n",
      "Epoch 491/500\n",
      "200/200 [==============================] - 0s 47us/sample - loss: 0.0040\n",
      "Epoch 492/500\n",
      "200/200 [==============================] - 0s 42us/sample - loss: 0.0040\n",
      "Epoch 493/500\n",
      "200/200 [==============================] - 0s 54us/sample - loss: 0.0040\n",
      "Epoch 494/500\n",
      "200/200 [==============================] - 0s 50us/sample - loss: 0.0040\n",
      "Epoch 495/500\n",
      "200/200 [==============================] - 0s 48us/sample - loss: 0.0040\n",
      "Epoch 496/500\n",
      "200/200 [==============================] - 0s 46us/sample - loss: 0.0040\n",
      "Epoch 497/500\n",
      "200/200 [==============================] - 0s 41us/sample - loss: 0.0039\n",
      "Epoch 498/500\n",
      "200/200 [==============================] - 0s 44us/sample - loss: 0.0039\n",
      "Epoch 499/500\n",
      "200/200 [==============================] - 0s 41us/sample - loss: 0.0039\n",
      "Epoch 500/500\n",
      "200/200 [==============================] - 0s 52us/sample - loss: 0.0039\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "<tensorflow.python.keras.callbacks.History at 0x7fe648049b50>"
      ]
     },
     "execution_count": 353,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "env_model.fit(xs, ys, epochs=500)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 354,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[<tf.Variable 'dense_49/kernel:0' shape=(2, 2) dtype=float32, numpy=\n",
       " array([[ 0.49919292, -0.7909084 ],\n",
       "        [ 0.77810705, -0.43811953]], dtype=float32)>,\n",
       " <tf.Variable 'dense_52/kernel:0' shape=(4, 2) dtype=float32, numpy=\n",
       " array([[ 0.03430925,  0.53939   ],\n",
       "        [-0.43841517, -0.6861578 ],\n",
       "        [-0.08981416, -0.15196005],\n",
       "        [-0.12835124, -0.09376138]], dtype=float32)>,\n",
       " <tf.Variable 'dense_52/bias:0' shape=(2,) dtype=float32, numpy=array([0.08414647, 0.07566677], dtype=float32)>]"
      ]
     },
     "execution_count": 354,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "env_model.weights"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 296,
   "metadata": {},
   "outputs": [],
   "source": [
    "class CuriosityWrapper(wrappers.PyEnvironmentBaseWrapper):\n",
    "    \"\"\"Adds a model loss component to the reward.\"\"\"\n",
    "\n",
    "    def __init__(self, env, model, alpha=1.0):\n",
    "        \"\"\"Initialize.\n",
    "        \n",
    "        Args:\n",
    "            env: tf.agents environment\n",
    "            model: keras model [observation + one-hot action] -> observation\n",
    "            alpha: how much l2 norm loss for the model to add to the reward  \n",
    "        \"\"\"\n",
    "        super(CuriosityWrapper, self).__init__(env)\n",
    "\n",
    "        # saved old time-step\n",
    "        self.old_step = None\n",
    "\n",
    "        # keras model taking [obs + one-hot action] and outputting next obs\n",
    "        self.model = model\n",
    "\n",
    "        def model_for_obs_and_action(obs, act):\n",
    "            \"\"\"Take observation and action as a number, return next observation.\"\"\"\n",
    "            z = [np.hstack([obs, encode_onehot(act, v_n)])]\n",
    "            z = np.array(z)\n",
    "            return self.model(z)\n",
    "        self.model_for_obs_and_action = model_for_obs_and_action\n",
    "\n",
    "        self.last_action = None\n",
    "        \n",
    "        self.alpha = alpha\n",
    "\n",
    "    def transform_step(self, step):\n",
    "        \"\"\"Replace a reward inside the step to a curiosity reward (r + model loss)\"\"\"\n",
    "        \n",
    "        # reward to add\n",
    "        r = 0\n",
    "\n",
    "        # resetting old step, if required\n",
    "        if step.step_type == 0:\n",
    "            self.old_step = None\n",
    "\n",
    "        # computing the curiosity reward\n",
    "        if self.old_step is not None:\n",
    "            # observation predicted by the model\n",
    "            pred_obs = self.model_for_obs_and_action(\n",
    "                self.old_step.observation,\n",
    "                self.last_action)\n",
    "            \n",
    "            # computing the reward as l2 norm for the difference\n",
    "            r = np.linalg.norm(pred_obs - step.observation, ord=1) / np.linalg.norm(step.observation, ord=1)\n",
    "            print(pred_obs, step.observation)\n",
    "        \n",
    "        # remembering previous step\n",
    "        self.old_step = step\n",
    "\n",
    "        # computing the next reward\n",
    "        total_reward = step.reward + r\n",
    "\n",
    "        # returning a step with modified reward\n",
    "        total_reward = np.asarray(total_reward,\n",
    "                                  dtype=np.asarray(step.reward).dtype)\n",
    "\n",
    "\n",
    "        return ts.TimeStep(step.step_type, total_reward, step.discount,\n",
    "                           step.observation)\n",
    "\n",
    "    def _reset(self):\n",
    "        return self.transform_step(self._env.reset())\n",
    "\n",
    "    def _step(self, action):\n",
    "        # saving the action for transform_step\n",
    "        self.last_action = action\n",
    "        return self.transform_step(self._env.step(action))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 297,
   "metadata": {},
   "outputs": [],
   "source": [
    "env = VectorIncrementEnvironmentTFAgents(v_n=v_n, v_k=v_k, v_seed=v_seed,\n",
    "                                        do_transform=do_transform)\n",
    "env = wrappers.TimeLimit(env, time_limit)\n",
    "env = CuriosityWrapper(env, env_model, alpha=1.0)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 298,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "TimeStep(step_type=array(0, dtype=int32), reward=array(0., dtype=float32), discount=array(1., dtype=float32), observation=array([0., 0.], dtype=float32))"
      ]
     },
     "execution_count": 298,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "env.reset()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 320,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "tf.Tensor([[-0.05536565 -0.02697793]], shape=(1, 2), dtype=float32) [-0.03624179 -0.02725032]\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "TimeStep(step_type=array(1, dtype=int32), reward=array(0.30120066, dtype=float32), discount=array(1., dtype=float32), observation=array([-0.03624179, -0.02725032], dtype=float32))"
      ]
     },
     "execution_count": 320,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "env.step(1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.7"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
