{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "from matplotlib import pyplot as plt\n",
    "import tensorflow as tf\n",
    "from tqdm import tqdm\n",
    "tf.compat.v1.enable_v2_behavior()\n",
    "\n",
    "from sklearn.decomposition import PCA"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from tf_agents.agents.dqn import dqn_agent\n",
    "from tf_agents.drivers import dynamic_step_driver\n",
    "from tf_agents.environments import suite_gym\n",
    "from tf_agents.environments import tf_py_environment\n",
    "from tf_agents.eval import metric_utils\n",
    "from tf_agents.metrics import tf_metrics, py_metrics\n",
    "from tf_agents.networks import q_network\n",
    "from tf_agents.policies import random_tf_policy, epsilon_greedy_policy\n",
    "from tf_agents.replay_buffers import tf_uniform_replay_buffer\n",
    "from tf_agents.trajectories import trajectory\n",
    "from tf_agents.utils import common\n",
    "from tf_agents.specs import array_spec\n",
    "from tf_agents.environments import utils, wrappers\n",
    "from tf_agents.trajectories import time_step as ts\n",
    "from tf_agents.drivers import dynamic_episode_driver\n",
    "from tf_agents.drivers import py_driver"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "class RandomSophisticatedFunction(object):\n",
    "    \"\"\"A function converting an input into a high-dimensional object.\"\"\"\n",
    "    def __init__(self, n=10, k=100, seed=11):\n",
    "        \n",
    "        tf.random.set_seed(seed)\n",
    "        \n",
    "        self.model = tf.keras.Sequential([\n",
    "            tf.keras.layers.Dense(50, input_shape=(n,)),\n",
    "          #  tf.keras.layers.Dense(100),\n",
    "            tf.keras.layers.Dense(k, bias_initializer='random_uniform'),\n",
    "        ])\n",
    "        \n",
    "    def __call__(self, x):\n",
    "        # !!! DISABLING THE FUNCTION\n",
    "        return np.array(x)\n",
    "        return self.model(np.array([x])).numpy()[0]\n",
    "    \n",
    "#assert RandomSophisticatedFunction(n=3, k=5, seed=1)([10,10,10]).shape == (5,)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "RandomSophisticatedFunction(n=3, seed=1)([10,10,10])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "class VectorIncrementEnvironment(object):\n",
    "    \"\"\"VectorIncrement environment.\"\"\"\n",
    "    def __init__(self, n=10, k=20):\n",
    "        self.n = n\n",
    "        self.k = k\n",
    "        self.e = RandomSophisticatedFunction(n=n, k=k)\n",
    "        self.s = np.zeros(self.n)\n",
    "\n",
    "    def encoded_state(self):\n",
    "        return np.array(self.e(self.s), dtype=np.float32)\n",
    "        \n",
    "    def reset(self):\n",
    "        self.s = np.zeros(self.n)\n",
    "        return self.encoded_state()\n",
    "\n",
    "    def step(self, action):\n",
    "        assert action in range(0, self.n + 1)\n",
    "        \n",
    "        s_old = np.copy(self.s)\n",
    "        if action > 0:\n",
    "            self.s[action - 1] += 1\n",
    "            \n",
    "            if max(s_old) - min(s_old) > 0:\n",
    "                r = (max(s_old) - s_old[action - 1]) / (max(s_old) - min(s_old))\n",
    "            else:\n",
    "                r = 0\n",
    "        else:\n",
    "            r = 0\n",
    "            \n",
    "        #def metric(x):\n",
    "        #    return max(x) - np.mean(x)\n",
    "       # \n",
    "       # r = (metric(s_old) - metric(self.s)) > 0\n",
    "        \n",
    "        \n",
    "            \n",
    "        return {'reward': float(r),\n",
    "               'state': np.copy(self.s),\n",
    "               'observation': self.encoded_state()}\n",
    "        \n",
    "        \n",
    "    def __repr__(self):\n",
    "        return str(self.s)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "n = 2\n",
    "env = VectorIncrementEnvironment(n=n, k=10)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "env.s"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "env.step(2)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "env.reset()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Testing if PCA works"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "obs = []\n",
    "states = []\n",
    "\n",
    "for _ in range(3):\n",
    "    o = env.reset()\n",
    "    for _ in range(50):\n",
    "        a = np.random.randint(0, n + 1)\n",
    "        result = env.step(a)\n",
    "        obs.append(result['observation'])\n",
    "        states.append(result['state'])\n",
    "states = np.array(states)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "pca = PCA(n_components=2)\n",
    "pca = pca.fit_transform(obs)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "plt.figure()\n",
    "plt.plot(pca[:, 0], pca[:, 1])\n",
    "plt.show()\n",
    "plt.figure()\n",
    "plt.plot(states[:, 0], states[:, 1])\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### PCA does not recover the variables we want...\n",
    "\n",
    "The increase is NOT monotonous"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Running RL with tf.agents"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "num_iterations = 20000 # @param {type: \"integer\"}\n",
    "\n",
    "initial_collect_steps = 1000 # @param {type: \"integer\"}\n",
    "collect_steps_per_iteration = 1\n",
    "replay_buffer_max_length = 1000\n",
    "\n",
    "batch_size = 256\n",
    "learning_rate = 1e-7\n",
    "log_interval = 1\n",
    "\n",
    "num_eval_episodes = 10\n",
    "eval_interval = 1"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "v_n = 2\n",
    "v_k = 2\n",
    "v_seed = 10"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "class VectorIncrementEnvironmentTFAgents(tf_py_environment.TFPyEnvironment):\n",
    "  def __init__(self):\n",
    "    self._action_spec = array_spec.BoundedArraySpec(\n",
    "        shape=(), dtype=np.int32, minimum=0, maximum=v_n, name='action')\n",
    "    self._observation_spec = array_spec.ArraySpec(\n",
    "        shape=(v_k,), dtype=np.float32, name='observation')\n",
    "    self._time_step_spec = ts.time_step_spec(self._observation_spec)\n",
    "    \n",
    "    self.env = VectorIncrementEnvironment(n=v_n, k=v_k)\n",
    "    self._state = self.env.encoded_state()\n",
    "    self._episode_ended = False\n",
    "    self._batched = False\n",
    "   # self._batch_size = None\n",
    "    \n",
    "  def batch_size(self):\n",
    "    return None\n",
    "\n",
    "  @property\n",
    "  def batched(self):\n",
    "    return False\n",
    "\n",
    "  def action_spec(self):\n",
    "    return self._action_spec\n",
    "\n",
    "  def observation_spec(self):\n",
    "    return self._observation_spec\n",
    "\n",
    "  def _reset(self):\n",
    "    self.env.reset()\n",
    "    self._state = self.env.encoded_state()\n",
    "    self._episode_ended = False\n",
    "    return ts.restart(self._state)\n",
    "\n",
    "  def _step(self, action):\n",
    "\n",
    "    if self._episode_ended:\n",
    "      # The last action ended the episode. Ignore the current action and start\n",
    "      # a new episode.\n",
    "      return self.reset()\n",
    "\n",
    "    res = self.env.step(action)\n",
    "    self._state = self.env.encoded_state()\n",
    "\n",
    "    return ts.transition(self._state, reward=res['reward'], discount=1.0)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# checking that the environment works\n",
    "env = VectorIncrementEnvironmentTFAgents()\n",
    "env = wrappers.TimeLimit(env, 10)\n",
    "utils.validate_py_environment(env, episodes=5)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "train_env = tf_py_environment.TFPyEnvironment(env)\n",
    "#train_env = env"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "fc_layer_params = (30, 30, 30, 30)\n",
    "\n",
    "q_net = q_network.QNetwork(\n",
    "    train_env.observation_spec(),\n",
    "    train_env.action_spec(),\n",
    "    fc_layer_params=fc_layer_params)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#optimizer = tf.compat.v1.train.AdamOptimizer(learning_rate=learning_rate)\n",
    "optimizer = tf.compat.v1.train.GradientDescentOptimizer(learning_rate=learning_rate)\n",
    "\n",
    "train_step_counter = tf.Variable(0)\n",
    "\n",
    "agent = dqn_agent.DqnAgent(\n",
    "    train_env.time_step_spec(),\n",
    "    train_env.action_spec(),\n",
    "    q_network=q_net,\n",
    "    optimizer=optimizer,\n",
    "    td_errors_loss_fn=common.element_wise_squared_loss,\n",
    "    train_step_counter=train_step_counter)\n",
    "\n",
    "agent.initialize()\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "replay_buffer = tf_uniform_replay_buffer.TFUniformReplayBuffer(\n",
    "    data_spec=agent.collect_data_spec,\n",
    "    batch_size=train_env.batch_size,\n",
    "    max_length=replay_buffer_max_length)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "num_episodes = tf_metrics.NumberOfEpisodes()\n",
    "env_steps = tf_metrics.EnvironmentSteps()\n",
    "avg_return = tf_metrics.AverageReturnMetric()\n",
    "act_hist = tf_metrics.ChosenActionHistogram()\n",
    "\n",
    "observers = [num_episodes, env_steps, avg_return, replay_buffer.add_batch, act_hist]\n",
    "\n",
    "#tf_policy = epsilon_greedy_policy.EpsilonGreedyPolicy(agent.collect_policy, 0.5)\n",
    "\n",
    "tf_env = train_env\n",
    "tf_policy = agent.collect_policy\n",
    "\n",
    "driver = dynamic_episode_driver.DynamicEpisodeDriver(\n",
    "    tf_env, tf_policy, observers, num_episodes=batch_size)\n",
    "\n",
    "# Initial driver.run will reset the environment and initialize the policy.\n",
    "final_time_step, policy_state = driver.run()\n",
    "\n",
    "print('final_time_step', final_time_step)\n",
    "print('Number of Steps: ', env_steps.result().numpy())\n",
    "print('Number of Episodes: ', num_episodes.result().numpy())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "avg_return.result()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "act_hist.result()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Dataset generates trajectories with shape [Bx2x...]\n",
    "dataset = replay_buffer.as_dataset(\n",
    "    num_parallel_calls=3, \n",
    "    sample_batch_size=batch_size, \n",
    "    num_steps=2).prefetch(3)\n",
    "\n",
    "iterator = iter(dataset)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "### (Optional) Optimize by wrapping some of the code in a graph using TF function.\n",
    "agent.train = common.function(agent.train)\n",
    "\n",
    "# Reset the train step\n",
    "agent.train_step_counter.assign(0)\n",
    "\n",
    "returns = []\n",
    "\n",
    "for _ in range(num_iterations):\n",
    "\n",
    "  # Collect a few steps using collect_policy and save to the replay buffer.\n",
    "  # Continue running from previous state\n",
    "  final_time_step, _ = driver.run(final_time_step, policy_state)\n",
    "\n",
    "  # Sample a batch of data from the buffer and update the agent's network.\n",
    "  experience, unused_info = next(iterator)\n",
    "  train_loss = agent.train(experience).loss\n",
    "\n",
    "  step = agent.train_step_counter.numpy()\n",
    "\n",
    "  if step % log_interval == 0:\n",
    "    print('step = {0}: loss = {1}'.format(step, train_loss))\n",
    "\n",
    "  if step % eval_interval == 0:\n",
    "    ret = avg_return.result()\n",
    "    print('step = {0}: Average Return = {1}'.format(step, ret))\n",
    "    avg_return.reset()\n",
    "    returns.append(ret)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "plt.plot(returns)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "replay_buffer.clear()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "num_episodes = tf_metrics.NumberOfEpisodes()\n",
    "env_steps = tf_metrics.EnvironmentSteps()\n",
    "avg_return = tf_metrics.AverageReturnMetric()\n",
    "act_hist = tf_metrics.ChosenActionHistogram()\n",
    "\n",
    "observers = [num_episodes, env_steps, avg_return, replay_buffer.add_batch, act_hist]\n",
    "\n",
    "#tf_policy = epsilon_greedy_policy.EpsilonGreedyPolicy(agent.collect_policy, 0.5)\n",
    "\n",
    "tf_env = train_env\n",
    "tf_policy = agent.policy\n",
    "\n",
    "driver = dynamic_episode_driver.DynamicEpisodeDriver(\n",
    "    tf_env, tf_policy, observers, num_episodes=batch_size)\n",
    "\n",
    "# Initial driver.run will reset the environment and initialize the policy.\n",
    "final_time_step, policy_state = driver.run()\n",
    "\n",
    "print('final_time_step', final_time_step)\n",
    "print('Number of Steps: ', env_steps.result().numpy())\n",
    "print('Number of Episodes: ', num_episodes.result().numpy())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "avg_return.result()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "replay_buffer.clear()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "for r in replay_buffer.gather_all().observation[:10].numpy()[0]:\n",
    "    print(r)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "xy = replay_buffer.gather_all().observation.numpy()[0]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "plt.hist(replay_buffer.gather_all().action.numpy()[0])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "replay_buffer.gather_all().action.numpy().shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "plt.plot(xy[:, 0], xy[:, 1])\n",
    "plt.plot([0, 10], [0, 10])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Hardcoded agent"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "o = train_env.reset().observation.numpy()[0]\n",
    "total_reward = 0\n",
    "while True:\n",
    "    act = 2 if o[0] > o[1] else 1\n",
    "    step = train_env.step(act)\n",
    "    o = step.observation.numpy()[0]\n",
    "    r = np.array(step.reward[0])\n",
    "    total_reward += r\n",
    "    print(act, o, r)\n",
    "    if step.step_type == 2:\n",
    "        break\n",
    "print(total_reward)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Write hardcoded agent and see what reward it gets!"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.7"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
