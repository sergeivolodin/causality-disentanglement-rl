import sparse_causal_model_learner_rl.trainable.decoder
import sparse_causal_model_learner_rl.trainable.reconstructor
import sparse_causal_model_learner_rl.trainable.model
import sparse_causal_model_learner_rl.trainable.gumbel_switch
import sparse_causal_model_learner_rl.config
import sparse_causal_model_learner_rl.annealer.threshold
import sparse_causal_model_learner_rl.loss.losses
import sparse_causal_model_learner_rl.loss.optimizer
import sparse_causal_model_learner_rl.metrics.nnz
import sparse_causal_model_learner_rl.metrics.loss_thresholded
import gin_tune
import sparse_causal_model_learner_rl.trainable.value_predictor
import sparse_causal_model_learner_rl.trainable.fcnet
import sparse_causal_model_learner_rl.learners.input_normalizer
include 'rec_nonlin_gnn.gin'

tune_run.num_samples = 1

Config.loss_every = 1000000000000000

model_gnn/FCNet.hidden_sizes = [5]
model_gnn/FCNet.activation_cls = @Sigmoid

model_gnn/WithInputSwitch.model_cls = @model_gnn/FCNet

LearnableSwitch.sample_many = False

Config.env_steps = 1000

Config.report_weights = True
Config.report_weights_every = 20

Config.collect_every = 100
Config.batch_training = True
Config.shuffle = True

decoder/FCNet.hidden_sizes = [150, 150, 150]
decoder/FCNet.activation_cls = @Tanh
ModelDecoder.model_cls = @decoder/FCNet
ModelDecoder.use_batchnorm = False#True

reconstructor/FCNet.hidden_sizes = [150, 150, 150]
reconstructor/FCNet.activation_cls = @Tanh
ModelReconstructor.model_cls = @reconstructor/FCNet

ManyNetworkModel.skip_connection = False
ManyNetworkModel.model_cls = @model_gnn/WithInputSwitch
ManyNetworkModel.sparse_do_max = False
ManyNetworkModel.sparse_do_max_mfma = False

Config.model = @ManyNetworkModel
Config.decoder = @ModelDecoder
Config.reconstructor = @ModelReconstructor

Config.feature_shape = (6, )

# normalize input data
Config.context_transforms = []

Config.losses = {'fit': {'fcn': @fit_loss, 'coeff': 1},
                 'sparsity': {'fcn': @sparsity_loss, 'coeff': 1e-10},
                 'reconstruction': {'fcn': @reconstruction_loss, 'coeff': 1},
                 'sbn': {'fcn': @soft_batchnorm_dec_out, 'coeff': 1.0}
                }


Config.execution = {'opt1': ['fit', 'reconstruction', 'sparsity', 'sbn']}#,'opt2': ['fit', 'reconstruction', 'sparsity'],'opt3': ['fit', 'reconstruction', 'sparsity']}


opt1/Optimizer.lr = 1e-2

Config.grad_clip_value = 1e-2